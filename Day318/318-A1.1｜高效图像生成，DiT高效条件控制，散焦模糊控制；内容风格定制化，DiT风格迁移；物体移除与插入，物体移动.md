# Topic: Image Generation｜Efficient｜Knowledge Distillation & DPO 

## LightGen: Efficient Image Generation through Knowledge Distillation and Direct Preference Optimization 
2025-03-11｜HKUST, Everlyn AI, UCF｜⭐️🟡 

<u>http://arxiv.org/abs/2503.08619v1</u>  
<u>https://github.com/XianfengWu01/LightGen</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/17.jpg)
本文提出了一种名为LightGen的高效图像生成模型，旨在通过知识蒸馏和直接偏好优化来减少对大规模数据集和高性能计算资源的依赖。传统的文本到图像生成模型通常需要庞大的数据集和复杂的架构，这限制了许多研究者和从业者的访问。LightGen采用了一种紧凑的Masked Autoregressive (MAR)架构，仅使用0.7亿参数，并通过生成2百万高质量的合成图像数据集来进行训练。研究表明，数据的多样性比数据量更能影响模型性能，从而显著降低了计算需求和预训练时间，提升了模型的可访问性。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/18.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/19.jpg)
LightGen的训练流程包括几个关键步骤：首先，使用变分自编码器（VAE）将真实图像编码为潜在表示，并将其分割为连续的图像令牌。接着，利用T5编码器处理文本，将其转换为高维嵌入，并通过交叉注意机制与图像令牌进行对齐。随后，Masked Autoencoder架构生成基于掩码输入的令牌预测，这些预测用于条件生成图像令牌。为了克服合成数据的局限性，LightGen引入了直接偏好优化（DPO）作为后处理技术，旨在提升生成图像的质量和位置准确性。通过优化生成图像与参考图像之间的差异，DPO进一步提高了模型在空间关系捕捉和高频细节处理方面的能力。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/20.jpg)
在实验部分，LightGen的性能通过与多种最先进的模型进行比较来评估。使用2百万图像的合成数据集，模型在256×256和512×512分辨率下的GenEval基准测试中表现出色，特别是在单对象、双对象和颜色任务中超过了多个对比模型。实验结果显示，LightGen在仅需88 GPU天的训练时间内达到了与大规模模型相当的性能。此外，DPO在后处理阶段的应用显著增强了模型在高频细节和空间关系处理上的效果。通过这些实验，LightGen展示出其在资源受限环境下的高效性和可行性。



### 通俗易懂
LightGen的工作流程可以简单理解为几个步骤。首先，真实的图像通过一种叫做变分自编码器的技术被转化为一种简化的表示形式，这样可以更容易地处理和生成图像。接下来，模型会将相关的文本信息转换成数字形式，以便与图像信息结合。然后，模型会根据这些信息生成新的图像。在这个过程中，LightGen还会使用一种叫做直接偏好优化的方法，来确保生成的图像不仅看起来好，而且在细节和位置上也更准确。最后，通过对比生成的图像和真实图像，模型不断调整自己，以提升生成图像的质量。这种高效的训练方式使得LightGen在资源有限的情况下也能产生高质量的图像。
# Topic: Image Generation｜Controllable｜Efficient
## OminiControl2: Efficient Conditioning for Diffusion Transformers 
2025-03-11｜NUS｜⭐️🟡 

<u>http://arxiv.org/abs/2503.08280v1</u>  
<u>https://github.com/Yuanshi9815/OminiControl</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/46.jpg)
OminiControl2是一个针对扩散变换器（Diffusion Transformers，DiT）模型的高效控制框架，旨在解决文本到图像生成中的计算效率问题。尽管OminiControl等先前方法在多种控制信号生成方面取得了进展，但在处理长条件输入时，仍面临显著的计算效率挑战。OminiControl2通过引入两项关键创新，显著提高了图像生成的效率：首先，动态压缩策略仅保留生成过程中最相关的条件标记，其次，条件特征重用机制仅在初始推理步骤中计算条件标记特征，并在后续步骤中重复使用。这种设计不仅减少了模型的参数数量，还保持了多模态输入的灵活性和生成质量，最终实现了在多条件生成场景中超过90%的计算开销降低。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/47.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/48.jpg)
OminiControl2的方法主要围绕两个核心原则展开：一是紧凑的标记表示，二是条件特征重用。具体来说，首先通过空间压缩策略减少条件图像的标记数量，从而优化计算效率。在此过程中，条件图像会被编码为潜在标记，形成统一的标记序列。其次，条件特征重用机制在初次推理时计算条件标记特征，并在后续去噪步骤中重复使用这些特征。为了应对多条件生成任务中的计算复杂性，OminiControl2还引入了一种非对称注意力机制，确保条件标记在去噪过程中的一致性，从而减少冗余计算。这些方法的结合使得OminiControl2在保持生成质量的同时，显著提高了计算效率。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/49.jpg)
在实验部分，OminiControl2的性能在多个条件生成任务中进行了评估，包括边缘到图像、深度到图像、掩模修复和图像去模糊等。实验结果表明，与原始OminiControl相比，OminiControl2在计算速度上实现了3.8到5.9倍的提升，尤其是在处理多个条件时更为明显。此外，OminiControl2在生成质量上也保持了竞争力，使用FID、CLIP和NIQE等指标进行评估，显示出在多条件场景下的优越表现。通过对比不同优化策略的效果，实验结果证明了紧凑标记表示和特征重用的有效性，进一步确认了该方法在实际应用中的潜力。



### 通俗易懂
OminiControl2的工作原理可以简单理解为“聪明地使用信息”。在生成图像时，这个系统会选择性地保留最重要的信息，而不是处理所有数据。首先，它通过压缩技术减少了需要处理的标记数量，这就像是把一大堆书籍整理成只保留对你最有用的几本。其次，它在第一次处理时计算出条件信息，然后在后续步骤中重复使用这些信息，就像你在做数学题时，第一次计算出结果后，后面就不再重复计算同样的内容。通过这样的方式，OminiControl2在生成图像时不仅快，而且质量也很高，特别是在需要多种条件的情况下，表现得更加出色。
# Topic: Image Generation｜Defocus Blur Control
## Bokeh Diffusion: Defocus Blur Control in Text-to-Image Diffusion Models 
2025-03-11｜NTU｜⭐️⭐️ 

<u>http://arxiv.org/abs/2503.08434v1</u>  
<u>https://atfortes.github.io/projects/bokeh-diffusion/</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/87.jpg)
Bokeh Diffusion是一种新颖的文本到图像扩散模型框架，旨在精确控制图像中的散景模糊效果。传统的摄影技术允许摄影师通过调整镜头光圈和焦距来塑造视觉美感，而当前的扩散模型通常依赖于提示工程来模拟这些效果，导致效果不尽如人意。Bokeh Diffusion通过明确地将扩散模型与物理散焦模糊参数相结合，提供了一种场景一致的散景控制方法。该方法不仅保留了场景的结构，还能根据不同的模糊程度生成清晰或模糊的图像。通过引入混合训练管道，该框架有效解决了缺乏成对真实图像的问题，使得模型能够在生成过程中更好地理解场景和模糊之间的关系。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/88.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/89.jpg)
Bokeh Diffusion的核心方法包括以下几个步骤：首先，利用真实世界图像与合成模糊增强的对齐，建立混合数据集，以提供丰富的对比散焦状态。其次，采用轻量级的交叉注意力机制，将散焦模糊条件注入到现有的文本到图像扩散管道中，从而不干扰场景的语义信息。第三，应用基础的自注意力机制，确保在不同模糊水平下场景布局的一致性。该机制通过选择一个随机的基准图像作为结构锚点，强制其他图像遵循一致的场景布局。最后，模型通过颜色转移技术调整不同模糊级别之间的颜色一致性，从而保持图像的自然感和真实感。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/90.jpg)
在实验部分，研究团队通过构建一个包含真实和合成图像的混合数据集，验证了Bokeh Diffusion的有效性。实验比较了该方法与三种主流文本到图像扩散模型（FLUX、SD3.5和SD1.5）的性能，评估了模糊控制的准确性和场景一致性。结果表明，Bokeh Diffusion在模糊趋势的相关性和图像一致性方面均显著优于基线模型。此外，研究还展示了该方法在生成图像和真实图像编辑中的灵活应用，表明其在实际场景中表现出色。通过定量和定性的评估，Bokeh Diffusion展现了强大的模糊调整能力和场景保持特性。



### 通俗易懂
Bokeh Diffusion的工作原理可以简单理解为一种智能图像处理工具，能够根据用户的需求调整照片的模糊程度。首先，它从大量真实照片中学习，了解不同模糊效果下的图像特征。然后，当用户希望某张照片看起来更清晰或更模糊时，系统会运用学习到的知识，自动调整图像的模糊程度，同时确保照片中的主要元素不会被改变。通过这种方式，用户可以在不影响整体画面的情况下，灵活地增加或减少模糊效果，就像摄影师通过调整相机设置来控制景深一样。这使得即使是普通用户也能轻松创建出具有专业水准的图像效果。
# Topic: Image Generation｜Customization 
## Modular Customization of Diffusion Models via Blockwise-Parameterized Low-Rank Adaptation 
2025-03-11｜CUHK, HKUST, HKU, Huawei, SmartMore｜⭐️🟡 

<u>http://arxiv.org/abs/2503.08575v1</u>  
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/21.jpg)
在近年来的研究中，扩散模型的定制化已取得显著进展，能够将用户提供的主题或风格概念有效地融入生成图像中。然而，如何高效地将多个去中心化训练的概念模块化组合成一个定制化模型，且不影响各自的身份特征，仍然是一个未解决的问题。现有的后训练方法通常局限于固定概念的组合，任何新的组合都需要重新训练，而即时合并方法则可能导致身份丢失和概念间的干扰。为了解决这些问题，本文提出了一种名为BlockLoRA的方法，旨在高效地合并多个概念，同时准确保留它们的身份特征。通过深入分析干扰的根源，本文开发了随机输出擦除技术，以最小化不同定制模型之间的干扰，并提出了块状LoRA参数化，以减少即时模型合并过程中的身份损失。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/22.jpg)
BlockLoRA方法的核心在于两个主要组成部分：随机输出擦除和块状LoRA参数化。具体方法如下：

1. **随机输出擦除**：该技术在定制过程中随机擦除LoRA输出的残余部分，使每个定制概念能够形成独立的特征分布，而不依赖于先前的类分布。这一过程通过引入噪声，迫使模型学习到每个概念的独特性。
2. **块状LoRA参数化**：为了有效合并多个LoRA，本文引入了二进制掩码，使得每个LoRA只优化参数的不同子集。这不仅减少了不同LoRA之间的方向干扰，还确保了参数的正交性。通过这种方式，BlockLoRA能够将多个概念的LoRA权重合并为一个模型，而不损失各自的身份特征。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/23.jpg)
为了验证BlockLoRA的有效性，本文进行了广泛的实验。实验使用了包含15个不同概念的数据集，涵盖了人物、风格和场景等多样性。对比方法包括Dreambooth LoRA、Mix-of-show和Orthogonal Adaptation等。评估指标主要为文本对齐度和图像对齐度，计算方法采用CLIP评分。实验结果表明，BlockLoRA在图像对齐度和文本对齐度上均优于现有的即时合并方法，且身份保留能力显著更强。尤其是在合并多个概念时，BlockLoRA表现出最小的身份损失。此外，用户偏好调查也显示，用户更倾向于使用BlockLoRA生成的图像，进一步证明了其在多概念定制化中的优越性。



### 通俗易懂
BlockLoRA方法可以简单理解为一种将多个概念组合在一起的“拼图技巧”。首先，它通过一种叫做随机输出擦除的技术，确保每个拼图块（即每个概念）在合并时不会互相干扰。想象一下，如果我们在拼图时，某些拼图块的形状与其他块重叠，可能会导致整个拼图看起来不协调。BlockLoRA通过随机擦除不必要的部分，确保每个拼图块的独特性。其次，块状LoRA参数化就像为每个拼图块分配特定的空间，这样它们就不会在合并时互相挤压，保持各自的形状。通过这种方法，BlockLoRA能够快速而有效地将多个概念合并成一个完整的图像，同时确保每个概念的特征和身份都能被保留。 
# Topic: Image Generation｜Style Transfer 
## U-StyDiT: Ultra-high Quality Artistic Style Transfer Using Diffusion Transformers 
2025-03-11｜ZJU, 360AI｜⭐️⭐️ 

<u>http://arxiv.org/abs/2503.08157v1</u>  
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/65.jpg)
U-StyDiT是一种创新的艺术风格迁移方法，旨在使用基于变换器的扩散模型生成超高质量的艺术风格化图像。传统的艺术风格迁移方法存在生成图像时出现明显伪影和不和谐图案的问题，限制了其在超高质量图像生成中的应用。U-StyDiT通过引入多视角风格调制器（MSM）和StyDiT模块，解决了内容与风格信息的有效学习与分离问题。该方法不仅能够从风格图像中提取全局和局部的风格信息，还能在生成过程中保持内容图像的结构完整性。此外，U-StyDiT还构建了一个超高质量艺术图像数据集Aes4M，包含400,000个高美学质量的艺术图像，进一步提高了风格迁移的效果。通过定性和定量实验，U-StyDiT在生成高质量艺术风格化图像方面显著优于现有的最先进方法。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/66.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/67.jpg)
U-StyDiT方法的核心在于两个主要模块：多视角风格调制器（MSM）和StyDiT块。具体方法如下：

1. **多视角风格调制器（MSM）**：
   - 从风格图像中提取全局和局部的风格信息。
   - 处理风格图像时，不会简单丢弃相似度较低的图像块，而是在token级别合并这些块以减少计算复杂度。
   
2. **StyDiT块**：
   - 同时学习内容和风格条件，以便在生成过程中有效分离这两种信息。
   - 通过设计一种新的训练流程，确保在扩散模型中同时引入内容和风格信息，从而生成更高质量的艺术图像。

3. **数据集Aes4M**：
   - 包含10个类别，每个类别400,000个艺术图像，确保高美学质量和文本-图像一致性。
   - 该数据集的构建考虑了清晰的Canny图像，以提高模型训练的有效性。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/68.jpg)
在实验部分，U-StyDiT的性能通过定性和定量指标进行评估。首先，使用结构相似性指数（SSIM）和CLIP评分来评估生成图像与内容图像之间的相似性。结果显示，U-StyDiT的SSIM和CLIP评分均高于现有的最先进方法，表明其在内容保留和风格一致性方面的优势。此外，使用美学评分模型对生成的图像进行评估，U-StyDiT获得了最高的美学评分，进一步证明了其在生成高质量艺术图像方面的能力。用户研究也显示，U-StyDiT生成的图像在用户偏好上优于其他方法，78%的参与者认为这些图像更接近人类创作的作品。总的来说，U-StyDiT在各项指标上均表现出色，验证了其有效性和优越性。



### 通俗易懂
U-StyDiT是一种新型的图像处理技术，旨在将一种艺术风格应用到另一幅图像上，生成高质量的艺术作品。它的工作原理可以分为两个主要部分。首先，它使用一个叫做多视角风格调制器的工具，从目标艺术作品中提取风格信息，而不是简单地丢弃一些看起来不太相似的部分。这样可以更全面地捕捉到风格的细节。其次，U-StyDiT通过一个名为StyDiT块的模块，能够同时理解和处理图像的内容和风格信息。这意味着在生成新图像时，它不会失去原始图像的结构。最后，U-StyDiT还建立了一个包含数百万艺术作品的数据集，以确保其生成的图像不仅美观，而且与原始内容保持一致。通过这些创新，U-StyDiT能够创造出更高质量的艺术风格化图像。 
## DyArtbank: Diverse Artistic Style Transfer via Pre-trained Stable Diffusion and Dynamic Style Prompt Artbank 
2025-03-11｜ZJU, CUHK｜⭐️🟡 

<u>http://arxiv.org/abs/2503.08392v1</u>  
<u>https://github.com/Jamie-Cheung/DyArtbank</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/33.jpg)
本研究提出了一种新颖的艺术风格迁移框架，称为DyArtbank，旨在生成多样化且高度真实的艺术风格化图像。传统的艺术风格迁移方法通常只能生成一致的艺术风格图像，缺乏多样性，限制了用户的创作空间。为了解决这一问题，DyArtbank引入了动态风格提示艺术库（DSPA），该库包含可学习的参数，能够从大量艺术作品中学习和存储风格信息，并动态指导预训练的稳定扩散模型生成多样化的艺术风格化图像。此外，研究还提出了关键内容特征提示（KCFP）模块，旨在为预训练模型提供足够的内容提示，以保持输入内容图像的详细结构。通过广泛的定性和定量实验，验证了DyArtbank在内容保留和风格多样性方面的有效性。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/34.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/35.jpg)
DyArtbank的方法主要包括两个核心模块：动态风格提示艺术库（DSPA）和关键内容特征提示（KCFP）。具体步骤如下：

1. **动态风格提示艺术库（DSPA）**：该模块包含一组可学习的参数，能够从艺术作品集中学习并存储风格信息。在训练阶段，DSPA从艺术作品中随机采样风格图像，并通过优化算法使这些参数收敛，以便在推理阶段动态采样，从而指导预训练的稳定扩散模型生成多样化的艺术风格图像。
2. **关键内容特征提示（KCFP）**：该模块通过微调的ControlNet提取输入内容图像的特征，并提供充分的内容提示，确保在生成过程中保持输入图像的结构。KCFP的训练是在内容图像上进行的，确保其能够有效地提取和提供内容特征。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/36.jpg)
本研究通过一系列实验评估DyArtbank的性能，主要包括定性和定量评估。定性实验通过对比DyArtbank与传统风格迁移方法（如ArtBank、CycleGAN等）的生成结果，展示了DyArtbank在内容结构保持和风格多样性方面的显著优势。定量实验则采用Fréchet Inception Distance（FID）等指标，测量生成图像与真实艺术图像之间的差异。实验结果表明，DyArtbank在保持内容结构的同时，能够生成多样化且高度真实的艺术风格化图像。此外，研究还探讨了数据增强的潜力，表明DSPA能够生成随机艺术图像样本，为模型训练提供更多样本。



### 通俗易懂
在DyArtbank的方法中，研究者们设计了两个主要部分来帮助计算机将艺术风格应用到普通图片上。首先是动态风格提示艺术库（DSPA），想象它像一个艺术品的数据库，计算机可以从中学习不同艺术家的风格。研究者们让计算机在这个数据库中随机选择风格，然后在生成新图像时使用这些风格，这样就能产生多种不同的艺术效果。第二个部分是关键内容特征提示（KCFP），这个模块的作用是确保计算机在生成艺术图像时，能够保留原始图片的主要结构。就像给计算机提供了一张地图，让它知道在创作时要保持哪些重要的部分。通过这两个部分，DyArtbank能够创造出既美观又多样的艺术作品。 
# Topic: Image Generation｜Editing｜Insertion-Removal, Object Movement 
## OmniPaint: Mastering Object-Oriented Editing via Disentangled Insertion-Removal Inpainting 
2025-03-11｜U Rochester, Adobe Research｜⭐️🟡 

<u>http://arxiv.org/abs/2503.08677v2</u>  
<u>https://yeates.github.io/OmniPaint-Page/</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/4.jpg)
在近年来，基于扩散模型的生成技术为图像编辑带来了革命性的进展，尤其是在物体移除和插入方面。然而，现有方法在真实场景中应用时仍面临诸多挑战，包括物理效果的复杂交互和缺乏足够的配对训练数据。为了解决这些问题，研究者们提出了OmniPaint，这一统一框架重新定义了物体移除与插入为相互依赖的过程，而非孤立的任务。OmniPaint结合了预训练的扩散模型和渐进式训练流程，能够精准地消除前景物体并无缝地插入新物体，同时忠实地保留场景的几何形状和内在属性。此外，研究团队还提出了一种新的无参考评估指标——上下文感知特征偏差（CFD），用于高保真图像编辑的质量评估，从而为物体导向的图像处理设立了新的基准。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/5.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/6.jpg)
OmniPaint的方法论主要包括两个关键方向：物体移除和物体插入。具体步骤如下：

1. **数据收集与增强**：收集3300个真实世界配对样本，应用不同的增强策略以提高模型的鲁棒性，包括形态变换和边界扰动等。
2. **训练流程**：
   - **预训练**：通过对LAION数据集进行随机遮挡训练，建立基础的图像修复能力。
   - **配对热身**：利用收集的配对样本分别训练物体移除和插入的模型，增强对物理效果的理解。
   - **CycleFlow无配对后训练**：利用大规模物体分割数据集进行无配对训练，提升模型的身份一致性和效果生成。
3. **上下文感知特征偏差（CFD）评分**：引入CFD评分以量化物体移除的性能，评估合成内容的真实性和与背景的融合程度，包括对虚假物体结构的惩罚和上下文一致性的评估。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/7.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/8.jpg)
在实验部分，研究者们对OmniPaint在物体移除和插入任务中的表现进行了全面评估。通过与多种现有方法的比较，OmniPaint在多个指标上表现优越，包括FID、SSIM和ReMOVE等。对于物体移除，OmniPaint能够有效去除物体及其物理效果，确保背景的无缝重建，且在处理复杂的光影效果时表现出色。对于物体插入，OmniPaint在保持插入物体的身份一致性和自然融合方面也取得了显著成果。实验结果表明，OmniPaint不仅提升了图像编辑的质量，还在减少不必要的伪影和保持视觉一致性方面设立了新的标准。



### 通俗易懂
在OmniPaint的工作中，研究者们通过一个简单的框架来处理图像中的物体移除和插入。首先，他们收集了大量真实的图像样本，并对这些图像进行了处理，以确保模型能够学习到如何在不同情况下有效地去掉不需要的物体，并把新物体自然地放入场景中。训练过程分为几个阶段，首先是基础训练，让模型学会填补空白区域，然后是用实际样本进行的细致训练，最后是利用大量未配对的数据来进一步提升模型的表现。为了评估效果，他们还设计了一种新的评分系统，能够检测是否在移除物体时产生了虚假的内容，并确保合成的图像与背景自然融合。这样，OmniPaint就能让图像编辑变得更加真实和高效。 
## ObjectMover: Generative Object Movement with Video Prior 
2025-03-11｜HKU, Adobe Research｜CVPR 2025｜⭐️⭐️ 

<u>http://arxiv.org/abs/2503.08037v1</u>  
<u>https://xinyu-andy.github.io/ObjMover</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/156.jpg)
在图像编辑领域，移动对象至另一个位置是一个看似简单但实际上极具挑战性的任务。此过程不仅需要重新协调光照，还要调整物体的姿态，准确填补被遮挡的区域，并确保阴影和反射的同步。然而，现有的方法在处理复杂场景时常常面临困难。为了解决这一问题，本文提出了ObjectMover，一个基于生成模型的物体移动框架。该框架通过将物体移动视为一个序列到序列的问题，并利用视频生成模型的知识，能够在复杂的现实场景中实现物体的高质量移动。我们通过大量实验验证了ObjectMover在真实世界场景中的适应性和出色表现，展示了其在光照和物体效果移动方面的能力。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/157.jpg)
ObjectMover的核心思想是将物体移动任务重新表述为一个序列到序列的预测问题。具体方法可分为以下几个步骤：首先，我们利用预训练的视频扩散模型作为基础，重新调整其以适应单帧图像生成任务。其次，通过使用现代游戏引擎（如虚幻引擎），构建一个合成数据生成管道，以生成高质量的训练数据对，确保数据的多样性和真实性。最后，采用多任务学习策略，将与物体移动相关的任务（如物体的移除和插入）整合在一起，以增强模型的泛化能力。通过这种方式，ObjectMover能够在处理光照变化、物体身份保持和场景一致性等方面表现出色。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/158.jpg)
在实验中，我们对ObjectMover进行了广泛的评估，使用了两个新的数据集ObjMove-A和ObjMove-B，以测试其在物体移动、移除和插入任务中的表现。ObjMove-A包含200个图像集，经过专业摄影师拍摄，确保了高质量的参考图像。通过与当前最先进的方法进行比较，ObjectMover在多个指标上均表现出色，特别是在保持物体身份和一致的光照效果方面。此外，我们还进行了用户研究，参与者普遍认为ObjectMover生成的图像更具自然感和真实感。实验结果证明了该方法在实际应用中的有效性和优越性。



### 通俗易懂
ObjectMover的工作原理可以简单理解为一个“智能视频编辑器”。它的目标是将图像中的某个物体移动到新的位置，同时确保周围的光照和影子看起来依然自然。首先，它会分析图像，识别出要移动的物体和其原始位置。接着，系统会根据物体的新位置，自动调整光线和影子，确保它们的变化是连贯的。为了训练这个系统，研究人员使用了一个游戏引擎生成大量的训练数据，这些数据帮助模型学习如何在不同的光照和场景条件下进行编辑。最后，ObjectMover还可以同时处理多个任务，比如移除物体和将物体放入新位置，这样用户就能得到更高质量的编辑结果，像是在制作一段短视频一样流畅自然。 
