# Topic: Video Generation

## WISA: World Simulator Assistant for Physics-Aware Text-to-Video Generation 
2025-03-11｜SYSU, Peng Cheng Lab, 360 AI｜⭐️⭐️ 

<u>http://arxiv.org/abs/2503.08153v1</u>  
<u>https://360cvgroup.github.io/WISA/</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/117.jpg)
随着文本到视频生成（T2V）技术的快速发展，现有模型在理解物理原理和生成符合物理规律的视频方面仍面临重大挑战。为了解决这一问题，研究者们提出了世界模拟助手（WISA），旨在将物理原则有效地整合到T2V模型中。WISA利用一个新的数据集WISA-32K，该数据集包含约32,000个视频片段，涵盖了17种物理现象，分为动力学、热力学和光学三个主要领域。通过对物理原理的分解和结构化，WISA能够帮助T2V模型更好地理解和生成与现实物理规律一致的视频。实验结果表明，WISA显著提高了生成视频的物理一致性，并在VideoPhy基准测试中取得了优异的表现。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/118.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/119.jpg)
WISA的核心在于将物理原则解构为文本描述、定性物理类别和定量物理属性，并通过特定的条件注入方法将这些信息嵌入到T2V模型中。具体方法包括：

1. **文本物理描述**：将物理原则与视频内容的文本描述结合，以生成符合物理规律的视觉效果。
2. **定性物理分类**：为每种物理现象分配专家头，利用混合物理专家注意力机制（MoPA）来专注于特定的物理类别，确保模型能够独立处理每种物理现象。
3. **定量物理属性**：将物理量（如密度、时间、温度）编码为物理嵌入，并通过自适应层归一化（AdaLN）注入到模型中，以增强模型的物理感知能力。此外，WISA还引入了物理分类器，以帮助模型理解和识别不同的物理现象。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/120.jpg)
在实验中，研究者选择了CogVideoX-5B作为基准T2V模型，以验证WISA的有效性。通过对比实验，WISA在物理一致性（PC）和语义一致性（SA）方面均表现出色。具体而言，WISA在VideoPhy基准测试中，SA和PC得分分别提高了0.07和0.05，显示出其在生成符合物理规律的视频方面的显著优势。此外，WISA-32K数据集的构建也经过了严格的筛选和标注，确保每个视频都清晰展示特定的物理现象。定性分析表明，WISA生成的视频在物理现象的表现上更为准确和一致，进一步验证了其在物理导向视频生成中的潜力。



### 通俗易懂
WISA的工作原理可以简单理解为一个“物理知识助手”，它帮助视频生成模型更好地理解物理现象。首先，WISA将复杂的物理原理拆分成易于理解的部分，比如用简单的文字描述物理现象、分类不同类型的物理现象（如碰撞、液体运动等），以及记录一些具体的物理量（如温度和时间）。接着，WISA会把这些信息注入到生成视频的模型里，确保模型在制作视频时能够遵循这些物理原则。比如，当模型生成一个水球爆炸的场景时，WISA会提醒它考虑水的流动和重力的影响。通过这种方式，WISA让视频生成模型不仅仅是“画画”，而是能够“理解”这些画面背后的物理规律，从而创造出更真实、更符合现实的动画。
## Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled Sampling 
2025-03-11｜KAIST, Adobe Research｜⭐️🟡 

<u>http://arxiv.org/abs/2503.08605v1</u>  
<u>https://syncos2025.github.io/</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/109.jpg)
在视频生成领域，传统的文本到视频（T2V）模型在生成高质量长视频时面临诸多挑战，尤其是由于计算成本高和缺乏足够的高质量长视频数据。为了解决这些问题，研究者们提出了Synchronized Coupled Sampling（SynCoS）这一新颖的推理框架。SynCoS通过同步的去噪路径和固定的基础噪声，确保了长视频生成过程中的一致性和流畅性。该框架能够有效地生成多事件长视频，克服了以往方法在长时间序列中常见的内容漂移和语义不一致的问题。实验结果表明，SynCoS在视频质量、时间一致性和提示保真度方面均显著优于现有的调优自由方法。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/110.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/111.jpg)
SynCoS的核心在于其三阶段的同步耦合采样过程。具体而言，这一过程包括以下几个关键步骤：

1. **时间去噪**：使用Denoising Diffusion Implicit Models（DDIM）对长视频进行分段，每段视频都被处理以确保局部平滑性。此过程通过将长视频分割为重叠的短视频块来实现。
2. **全局一致性优化**：在第一阶段生成的局部输出基础上，采用Collaborative Score Distillation（CSD）对输出进行全局优化，以确保不同视频块之间的语义一致性。
3. **反馈机制**：将优化后的输出反馈到前一个时间步，进一步提升生成质量。此阶段通过固定的基础噪声来稳定优化过程，确保不同视频块之间的提示指导不被削弱。

通过这三个阶段的结合，SynCoS能够在生成过程中实现平滑的局部过渡与全局一致性，为多事件长视频生成提供了强有力的支持。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/112.jpg)
在实验部分，研究者对SynCoS进行了全面的评估，涵盖了多种挑战性场景和不同的T2V模型。实验设置包括48个长视频场景，这些场景的长度比基线模型长出4到5倍，涉及对象运动、相机控制、背景变化等多种动态元素。SynCoS的表现与现有的调优自由基线（如Gen-L-Video和FIFO-Diffusion）进行了对比，结果显示在时间一致性、每帧图像质量和提示保真度等方面，SynCoS均显著优于这些基线。此外，研究还进行了消融实验，以验证每个阶段和关键组件对生成质量的影响，结果表明，去噪路径的同步和结构化提示在保持语义一致性和生成动态视频方面至关重要。



### 通俗易懂
在SynCoS的方法中，生成长视频的过程可以被看作是一个分步的工作。首先，系统将一个长视频分成几个小段，每段都被单独处理，以确保每一段之间的过渡是平滑的。这就像在制作电影时，先拍摄每个场景，然后再将它们拼接在一起。接下来，系统会对这些小段进行整体优化，确保它们在内容和风格上保持一致，避免出现任何不协调的变化。最后，系统会将这些小段的结果反馈到之前的步骤，进一步改善生成的质量。这个过程确保了视频在长时间播放时，内容的连贯性和视觉效果都能保持在高水平。 
## SKALD: Learning-Based Shot Assembly for Coherent Multi-Shot Video Creation 
2025-03-11｜Purdue U, Adobe Research｜⭐️🟡 

<u>http://arxiv.org/abs/2503.08010v1</u>  
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/121.jpg)
本研究提出了SKALD，一个基于学习的多镜头视频组装方法，旨在从候选镜头中构建连贯的视频序列，尤其是在缺乏文本信息的情况下。SKALD的核心是学习剪辑组装（LCA）评分，这是一种度量镜头之间时序和语义关系的学习型指标，用于量化叙事连贯性。为了应对多镜头组合的指数复杂性，研究团队设计了一种高效的束搜索算法，以LCA评分为指导。此外，为了有效地训练模型，研究者提出了两项任务：镜头连贯性学习，通过对比学习来区分连贯与不连贯的序列，以及特征回归，将学习到的表示转换为实际的连贯性评分。实验结果表明，SKALD在VSPD和自定义的MSV3C数据集上相较于现有方法提高了高达48.6%的交并比（IoU）和43%的推理速度。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/122.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/123.jpg)
SKALD的设计包含两个主要组成部分：学习剪辑组装（LCA）评分和高效的组装算法。LCA评分是专为评估多镜头连贯性而设计的度量，建模镜头间的时序和空间关系。以下是方法的具体步骤：

1. **问题定义**：定义多镜头视频组装任务，输入为n个镜头，每个镜头有k个候选镜头，目标是组装出最连贯的镜头序列。
2. **数据准备**：利用经过编辑的视频作为正样本，通过轻微扰动生成正样本，构建与不连贯内容的负样本。
3. **LCA评分学习**：采用对比学习优化LCA编码器，区分连贯和不连贯序列，并通过回归任务将特征映射到实际评分。
4. **束搜索算法**：通过束搜索有效探索候选镜头组合，避免了穷举搜索的高复杂度，确保高效的镜头选择。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/124.jpg)
在实验部分，SKALD在两个数据集上进行了评估，以验证其有效性和适用性。首先，在VSPD数据集上，SKALD与现有的文本驱动方法进行了比较，结果显示其在视频质量指标（如IoU和序列匹配得分）上均有显著提升，且推理时间减少了43%。其次，在MSV3C测试集上，SKALD通过集成文本信息的变体SKALD-text进一步提高了性能，显示出在缺乏详尽文本注释的情况下，SKALD仍能有效组装高质量的视频序列。此外，用户研究显示，参与者对SKALD组装的视频质量有明显偏好，进一步验证了该方法的实用性和效果。



### 通俗易懂
SKALD的工作原理可以简单理解为一个智能视频编辑器。首先，它会根据用户的需求，选择多个视频片段（镜头），然后通过一种叫做LCA评分的方式，评估这些镜头之间的连贯性。具体来说，SKALD会检查每个镜头如何与其他镜头相连接，确保它们在时间和内容上是和谐一致的。接下来，SKALD使用一种高效的束搜索方法来选择最佳的镜头组合，而不是尝试所有可能的组合，这样可以节省大量时间。最后，为了训练这个系统，SKALD从已经编辑好的视频中学习，知道哪些镜头组合是流畅的，哪些是突兀的。通过这种方式，即使没有详细的文本说明，SKALD也能创造出高质量的连贯视频。 

## REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder 
2025-03-11｜Adobe Research, NEU｜⭐️🟡 

<u>http://arxiv.org/abs/2503.08665v1</u>  
<u>https://bespontaneous.github.io/REGEN/</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/104.jpg)
本研究提出了一种新颖的视频嵌入学习方法REGEN，旨在通过引入（重）生成解码器来提高视频生成模型的压缩效率。与传统的编码-解码器架构不同，REGEN采用了编码-生成器框架，利用扩散变换器（DiT）从紧凑的潜在空间合成缺失的细节。该方法的核心在于一个专门的潜在条件模块，以便有效地将编码视频的潜在嵌入作为生成解码器的条件信号。通过这种方式，REGEN能够实现高达32倍的时间压缩比，显著优于现有的主流视频嵌入方法，同时在生成高保真视频内容时保持了重建质量和效率的平衡。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/105.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/106.jpg)
REGEN的核心方法包括两个主要组件：时空视频编码器和基于DiT的生成解码器。首先，时空视频编码器负责将输入视频序列编码为紧凑的潜在空间，采用因果3D卷积块以保持时序关系。该编码器生成两个潜在帧：内容潜在帧（z_c）和运动潜在帧（z_m），分别捕捉视频的静态和动态信息。其次，生成解码器利用DiT架构，将潜在特征解码回像素空间，采用条件扩散过程来生成目标视频。为了提升解码器的灵活性，研究者设计了一种新的潜在条件机制，能够在不同的输入尺寸和纵横比下进行有效的映射和生成，进而支持视频的重建、插值和外推等多种任务。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/107.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/108.jpg)
在实验部分，研究者评估了REGEN在不同时间压缩比下的生成效果，并验证了其紧凑潜在空间在视频生成中的兼容性。通过在MCL-JCV和DAVIS 2019数据集上进行重建质量评估，采用了标准的量化指标如PSNR、SSIM和Frechet视频距离（rFVD）来衡量性能。实验结果显示，REGEN在8×8×4和8×8×32等不同压缩率下均优于现有的主流视频嵌入方法，尤其在高压缩比条件下，其重建质量显著提升。此外，REGEN还在文本到视频生成任务中展现了良好的适应性，成功生成了高质量的视频内容，证明了其在实际应用中的潜力。



### 通俗易懂
REGEN是一种新型的视频处理方法，它通过将视频压缩得更小，同时保持良好的画质，来提高视频生成的效率。其工作原理分为两个主要步骤。首先，REGEN使用一种特殊的编码器将视频分解成两部分：一部分是内容信息（比如画面的静态部分），另一部分是运动信息（比如画面的动态变化）。接着，REGEN利用一个强大的解码器，将这些信息重新组合，生成高质量的视频。这个解码器的特别之处在于它能够根据不同的视频大小和形状进行灵活调整，确保生成的视频在各种条件下都能保持良好的效果。通过这种方法，REGEN不仅能在压缩视频时节省存储空间，还能在生成新视频时保证画质，展现出很好的应用前景。 
# Topic: Video & Audio Generation

## FilmComposer: LLM-Driven Music Production for Silent Film Clips 
2025-03-11｜SHU, SECMPE｜⭐️⭐️ 

<u>http://arxiv.org/abs/2503.08147v1</u>  
<u>https://apple-jun.github.io/FilmComposer.github.io/</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/130.jpg)
本研究提出了FilmComposer，一个基于大型语言模型（LLM）驱动的音乐制作框架，旨在为无声电影片段创作音乐。FilmComposer结合了生成模型与多代理系统，模拟专业音乐人的工作流程，强调音质、音乐性和音乐发展等核心要素。通过引入节奏、语义和视觉等多种控制方式，FilmComposer不仅提高了音乐创作的专业性，还使非专业用户能够轻松创作高质量音乐。我们构建了一个包含7418个电影片段及其对应音乐、描述和节奏点的数据集MusicPro-7k，以解决现有模型在音质和音乐性方面的不足。实验结果表明，FilmComposer在音质、一致性、音乐性和音乐发展等方面均表现优异，超越了现有技术。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/131.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/132.jpg)
FilmComposer的工作流程分为三个主要模块：视觉处理、节奏可控的MusicGen和多代理评估、编排与混音。具体步骤如下：

1. **视觉处理**：对输入的电影片段进行分析，提取节奏条件、视觉语言和动作描述，模拟音乐创作的分析和定位过程。
2. **节奏可控的MusicGen**：生成与节奏点和视觉描述相符的主旋律。该模块使用节奏调节器和T5文本编码器，确保生成的旋律具备高音乐性和与视频的紧密对接。
3. **多代理评估、编排与混音**：通过多代理系统对生成的旋律进行评估，确保其符合音乐理论标准。随后，代理系统负责安排和混音，最终输出高质量的音乐作品。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/133.jpg)
在实验中，我们使用MusicPro-7k数据集对FilmComposer进行训练和评估。主要评估指标包括节奏控制、动态变化和乐器编配。通过使用Madmom提取节拍点并进行交叉相关分析，评估节奏控制的效果。动态变化则通过分贝变化来表示，较低的余弦相似度值表明更好的动态变化相似性。乐器编配通过Musicnn模型提取乐器分布，较低的余弦相似度值表明乐器编配的相似性。实验结果显示，FilmComposer在音乐质量、音频一致性和多样性等方面均超过了现有技术，证明了其在电影音乐制作中的有效性。



### 通俗易懂
FilmComposer的工作方式可以简单理解为一个音乐创作的自动助手。首先，它会分析电影片段的内容，比如画面中的动作和情感，然后根据这些信息生成适合的音乐节奏。接着，它会利用先进的技术生成旋律，确保旋律既好听又能与电影的节奏匹配。最后，FilmComposer会将这些旋律进行整理和混合，输出一段高质量的音乐。这一过程不仅能帮助专业音乐人提高效率，还能让普通用户也能轻松创作出动人的音乐作品。总之，FilmComposer就像一个智能音乐制作工具，帮助人们为无声电影增添生动的音乐。 
## $^R$FLAV: Rolling Flow matching for infinite Audio Video generation 
2025-03-11｜U Parma, U Siena｜⭐️ 

<u>http://arxiv.org/abs/2503.08307v2</u>  
<u>https://github</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/125.jpg)
本文提出了一种新颖的音视频生成架构RFLAV，旨在解决当前生成模型在音视频同步和质量上的主要挑战。尽管近年来在生成AI领域取得了显著进展，但实现高质量的音视频联合生成仍然面临诸多困难。RFLAV架构的设计考虑了三个关键要求：生成样本的质量、音视频的无缝同步与时间一致性、以及无限制的视频时长。通过引入轻量级的时间融合模块和三种不同的跨模态交互模块，RFLAV有效地提高了音视频生成的性能。实验结果表明，RFLAV在多模态音视频生成任务中优于现有的最先进模型，提供了更高的生成质量和更好的音视频一致性。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/126.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/127.jpg)
RFLAV架构采用了基于流匹配的滚动扩散方法，允许生成任意长度的音视频序列。其主要方法步骤包括：

1. **架构设计**：RFLAV由两个并行分支组成，分别处理视频和音频，避免在早期阶段进行模态间融合。
2. **视频和音频编码**：视频通过图像编码器逐帧生成，音频则通过梅尔谱图表示，确保音视频帧之间的一对一映射。
3. **跨模态交互**：提出三种不同的轻量级交互模块，分别通过自注意力机制和时间平均调制来实现音频与视频特征的融合。
4. **滚动流匹配**：采用滑动窗口去噪技术，逐步生成清晰的音视频帧，避免了自回归方法可能导致的质量下降。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/128.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/129.jpg)
为了验证RFLAV的有效性，研究者在两个数据集（Landscape和AIST++）上进行了广泛的实验。模型由12个RFLAV模块组成，窗口大小设定为10帧。通过对生成样本的定量评估，使用Frechet视频距离（FVD）、Kernel视频距离（KVD）和Frechet音频距离（FAD）等指标，RFLAV在多个任务中表现出色。与现有的最先进模型相比，RFLAV在生成质量、时间一致性和音视频同步方面均取得了显著的提升。此外，用户研究表明，在音视频质量和对齐性方面，RFLAV的表现也得到了参与者的高度认可。



### 通俗易懂
RFLAV是一种新型的音视频生成模型，它能同时生成高质量的视频和音频，并且没有长度限制。其工作原理可以分为几个简单的步骤：首先，模型将视频和音频分别处理，这样可以更好地理解和生成每种媒体。接着，模型通过一种叫做“滚动流匹配”的技术，逐帧生成视频和音频，而不是一次性生成所有内容，这样可以提高生成的质量。最后，模型在不同的时间点对音视频进行融合，确保它们在生成时保持同步。这种方法让RFLAV在生成音视频时，能够保持高质量和一致性，给用户带来更好的体验。 
