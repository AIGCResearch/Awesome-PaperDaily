# Topic: Image Detection｜Person 

## Referring to Any Person 
2025-03-11｜IDEA, SCUT｜⭐️🟡 

<u>http://arxiv.org/abs/2503.08507v1</u>  
<u>https://deepdataspace.com/blog/dino-xseek</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/218.jpg)
本研究聚焦于“指代任何人”这一任务，旨在通过自然语言描述检测图像中所有符合条件的个体。现有模型在实际应用中表现不佳，尤其是在多实例检测方面。为此，本文从任务定义、数据集设计和模型架构三个关键角度出发，提出了一种新方法。我们首先明确了可指代实体的五个方面以及该任务的三大特征。接着，开发了HumanRef数据集，以更好地反映真实场景中的需求，解决现有基准的局限性。最后，设计了RexSeek模型，结合多模态大语言模型与目标检测框架，旨在提升对复杂描述的理解和检测能力。实验结果表明，RexSeek在识别多个个体方面优于现有模型，具有更强的泛化能力。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/219.jpg)
在方法部分，我们从任务定义和模型设计两方面进行详细探讨。首先，任务定义明确了五个关键属性：1) 属性，涵盖性别、年龄、动作等；2) 位置，描述个体之间及与环境的空间关系；3) 互动，考虑人与人、人与物体及人与环境的交互；4) 推理，涉及多步推理以解决复杂的指代问题；5) 名人识别，识别特定个体。其次，从模型设计角度来看，RexSeek模型具备两大核心特性：1) 强大的感知能力，能够检测图像中的所有个体；2) 强大的语言理解能力，能有效解析复杂的自然语言描述。RexSeek通过整合先进的目标检测技术与多模态语言模型，形成一个高效的检索式模型，确保对多实例指代的准确识别。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/220.jpg)
在实验部分，我们通过多项评估指标对RexSeek模型进行了全面测试。实验结果显示，尽管许多现有模型在传统基准上表现优异，但在HumanRef数据集上，性能显著下降，主要原因在于这些模型通常只适应单实例检测。RexSeek在多实例指代方面表现出色，显示出其在真实场景下的应用潜力。此外，我们还进行了消融实验，以验证多阶段训练对模型性能的影响。结果表明，RexSeek在经过多阶段训练后，能够更好地处理复杂的指代任务，并在各种评估标准中取得了优异的成绩。这些实验结果强调了数据集设计和训练策略在提升指代表达模型可用性的重要性。



### 通俗易懂
在方法部分，我们通过两方面来解释如何进行“指代任何人”的任务。首先，我们明确了任务的定义，主要是通过描述来识别图像中符合条件的人。我们列出了五个关键点，比如描述一个人的性别、年龄、他们的动作、他们在图中的位置，以及他们与其他人的互动。接下来，我们设计了一个叫RexSeek的模型，它有两个主要的功能：一是能够准确检测图像中的所有人，二是能理解复杂的语言描述。RexSeek结合了图像识别和语言处理的技术，使得它在处理多个指代时表现得更好，能够在实际应用中更有效地找到和识别图像中的个体。 
# Topic: Multi-modal｜Image｜Comic Strips Understanding, Pixel Understanding, Captioning
## ComicsPAP: understanding comic strips by picking the correct panel 
2025-03-11｜UAB, U Florence｜⭐️🟡 

<u>http://arxiv.org/abs/2503.08561v1</u>  
<u>https://github.com/llabres/ComicsPAP</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/24.jpg)
本研究介绍了ComicsPAP，一个针对漫画条理解的基准数据集，旨在填补大型多模态模型（LMMs）在处理漫画叙事中的不足。尽管LMMs在图像描述、视觉问答和视频理解等任务中取得了显著进展，但在解析漫画条时却面临挑战。这是因为漫画的叙事结构独特，要求模型理解面板之间的时间和空间关系。ComicsPAP数据集包含超过10万个样本，结构化为五个子任务，采用“选择面板”的框架，要求模型识别缺失的面板。通过对当前LMMs的评估，发现它们在这些任务中的表现接近随机水平，揭示了其在捕捉序列和上下文依赖性方面的显著局限性。为此，研究者对LMMs进行了适应性调整，以提高其在ComicsPAP上的表现，显示出该数据集为未来多模态漫画理解研究提供了坚实基础。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/25.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/26.jpg)
在方法部分，研究团队设计了ComicsPAP数据集，专注于评估三种主要的多模态理解技能：叙事预期、多面板推理和共指解析。具体方法包括：

1. **叙事预期（Closure Tasks）**：模型需从三个连续的上下文面板中选择下一个面板，分为角色连贯性、视觉闭合和文本闭合三种变体，确保选项来自同一叙事段落。
2. **多面板推理（Sequence Filling Task）**：通过滑动窗口技术，模型需在五个面板中找出一个缺失的面板，考察其对因果和时间推理的综合理解能力。
3. **共指解析（Caption Relevance Task）**：模型需将给定的文本说明与相应的面板关联，强调隐含的角色和情节关系，确保捕捉到场景转换中的语义连贯性。

这些任务以多类分类问题的形式构建，旨在全面评估模型在漫画理解中的不同能力。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/27.jpg)
在实验部分，研究团队评估了多种LMMs在ComicsPAP任务上的表现。首先，进行了零-shot实验，评估了不同参数规模的模型，包括SmolVLM和Qwen系列。结果显示，大多数模型的表现接近随机水平，尤其是SmolVLM模型，其在理解漫画面板时的准确率极低。为了解决这一问题，研究团队对部分模型进行了微调，使用自动生成的训练样本进行监督学习。微调后，模型的性能有了显著提升，尤其是Qwen2.5-VL系列模型，在多个任务上超越了零-shot的基线。微调的过程中，模型同时接触不同任务的样本，提升了其在漫画理解中的能力。这些实验结果不仅展示了当前LMMs的局限性，也强调了针对特定任务进行微调的重要性。



### 通俗易懂
在我们的研究中，我们提出了一种新的方法来帮助计算机更好地理解漫画。我们创建了一个名为ComicsPAP的数据集，里面有很多漫画样本，并设计了三种主要的任务来测试计算机的理解能力。首先，我们让计算机从三个连续面板中选择下一个面板，这样可以测试它是否能理解故事的连续性。其次，我们让计算机在五个面板中找出缺失的一个，考察它是否能推断出缺失的内容。最后，我们让计算机根据给定的文字说明找到对应的面板，这样可以测试它是否能理解角色和情节之间的关系。通过这些任务，我们希望计算机能像人类一样理解漫画的叙事方式。 
## SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by Imitating Human Annotator Trajectories 
2025-03-11｜ZJU, Ant Group｜CVPR 2025｜⭐️ 

<u>http://arxiv.org/abs/2503.08625v1</u>  
<u>https://github.com/aim-uofa/SegAgent</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/202.jpg)
本研究提出了一种新的多模态大语言模型（MLLM）评估框架——人类样式掩膜标注任务（HLMAT），旨在提升MLLM在像素级理解能力上的表现。尽管现有的MLLM在视觉任务中取得了一定进展，但在复杂的像素级理解上仍然存在显著不足。传统的评估方法往往过于粗糙，无法有效衡量模型的细粒度像素理解能力。HLMAT通过模拟人类标注者的交互式标注过程，构建了一个多步骤的马尔可夫决策过程，使得MLLM能够生成文本格式的点击点，从而实现高质量的掩膜生成。通过这种方法，我们开发了SegAgent模型，并在多个分割任务中展示了其与最先进方法相当的性能。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/203.jpg)
在方法部分，我们首先设计了HLMAT框架，使MLLM能够模仿人类标注者的标注轨迹。具体步骤如下：

1. **数据生成**：利用现有的标注数据集，构建人类标注者的轨迹数据集。通过自动化算法，模拟人类标注者在图像上的点击操作，生成一系列状态-动作对。
2. **模型微调**：使用生成的轨迹数据集对MLLM进行微调，使其能够在给定图像和文本提示的情况下，生成相应的掩膜。模型的输出为基于文本的坐标，便于与其他MLLM进行公平比较。
3. **决策增强**：引入强化学习策略，如StaR+和基于过程奖励模型（PRM）的树搜索，进一步提升模型在复杂分割任务中的鲁棒性和决策能力。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/204.jpg)
在实验部分，我们评估了SegAgent在HLMAT任务上的表现，并与其他最先进的方法进行了比较。实验主要分为两个阶段：首先在标准的引用表达分割（RES）数据集上进行比较，随后在我们提出的高质量引用表达分割（HRES）数据集上进行验证。结果显示，SegAgent在各种分割任务中都表现出色，尤其是在掩膜细化和标注过滤方面。此外，通过引入PRM和树搜索策略，模型在复杂场景中的性能得到了显著提升。这些实验结果表明，HLMAT为评估和提升MLLM的像素级理解能力提供了有效的框架。



### 通俗易懂
在我们的研究中，我们创造了一种新的方法，帮助计算机更好地理解图像中的细节。我们通过模拟人类标注者的工作，让计算机一步一步地学习如何在图像上进行标注。首先，我们利用已有的图像和标注数据，自动生成一些人类标注者的点击轨迹，这些轨迹就像是标注者在图像上点击的记录。接着，我们用这些轨迹来训练计算机，使它能够根据图像和文字提示来生成对应的标注。最后，我们还引入了一些智能决策的方法，帮助计算机在处理复杂图像时做出更好的选择。通过这些步骤，我们的计算机模型在处理图像标注任务时表现得更加出色。 
## Painting with Words: Elevating Detailed Image Captioning with Benchmark and Alignment Learning 
2025-03-10｜ByteDance｜ICLR 2025｜⭐️ 

<u>http://arxiv.org/abs/2503.07906v1</u>  
<u>https://github.com/MAGAer13/DeCapBench</u> 
### 概述 

在视觉理解领域，图像描述一直是一个关键任务，近年来，视觉语言模型（VLMs）的进步显著提升了生成详细图像描述的能力。然而，现有的详细图像描述评估仍然存在不足，主要体现在过时的评估指标和粗略的标注上。本文提出了DECAPBENCH和一种新颖的评估指标DCSCORE，专门针对详细描述任务而设计。DCSCORE通过将生成的描述分解为最小自给单元（原始信息单元）进行评估，关注描述的准确性和细致程度。我们的评估结果表明，DCSCORE与人类判断的相关性显著高于其他基于规则或模型的指标，同时，DECAPBENCH在描述任务上与VLM领域的结果高度相关，超越了现有的基准。此外，我们还提出了一种自动化的细粒度反馈收集方法FEEDQUILL，显示出在偏好优化中的强大泛化能力。通过对多种VLM进行广泛实验，我们的方法显著减少了虚假信息的产生，并在多个基准上提升了性能，超越了GPT-4o的表现。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/224.jpg)
本文提出的DCSCORE评估指标和DECAPBENCH基准旨在解决传统图像描述评估中的不足。DCSCORE的评估过程分为三个步骤：首先，分解（Decomposition），将生成的描述拆分为原始信息单元，确保每个单元都是自给自足的；其次，匹配（Matching），评估生成的描述与参考描述之间的重叠情况，确保生成的描述涵盖所有重要元素；最后，验证（Verification），使用现代VLMs（如GPT-4o）对每个原始信息单元进行正确性验证。通过这些步骤，我们能够更准确地评估描述的质量，特别是关注其细致性和全面性。此外，FEEDQUILL方法通过生成多个候选响应并逐一验证，从中收集偏好数据，优化模型的输出质量。这种方法不仅提高了评估的透明度，还增强了模型在生成详细图像描述时的表现。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/225.jpg)
在实验部分，我们基于不同规模和能力的LLaVA模型进行了一系列实验，验证所提出方法的有效性。我们使用多种数据集进行训练，包括MSCOCO和OpenImages，以确保模型学习到广泛的图像分布。实验结果表明，采用FEEDQUILL方法的模型在多个图像描述基准上表现优异，显著降低了虚假信息的生成。此外，我们还通过对不同偏好数据集的比较，展示了FEEDQUILL在偏好优化中的优势。实验表明，随着训练数据量的增加，模型在下游任务中的表现持续提升，特别是在DECAPBENCH和mmHal-V等基准上，FEEDQUILL方法的引入使得模型的性能提升显著。最终，我们的模型不仅在详细图像描述生成上超越了GPT-4o，还在视觉对话任务中表现出色，证明了其潜力和有效性。



### 通俗易懂
在本文中，我们提出了一种新的方法来评估和改进图像描述的质量。首先，我们将图像描述拆分成更小的部分，称为原始信息单元，这样可以更清楚地看到每个部分是否准确。接下来，我们比较这些小部分与参考描述之间的重合度，确保生成的描述包含所有重要信息。最后，我们使用强大的语言模型来验证这些小部分的准确性。为了让模型更好地学习，我们还采用了一种叫FEEDQUILL的方法，通过生成多个描述并逐一验证，收集反馈来调整模型的输出。这种方法可以帮助模型生成更详细和准确的图像描述，从而提升其在实际应用中的表现。通过这些步骤，我们的研究不仅提高了评估的准确性，还让模型在生成图像描述时表现得更加出色。 
# Topic: Multi-modal｜Video｜Long Video Understanding,  Egocentric Videos Understanding
## QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long Video Comprehension 
2025-03-11｜XMU, NJU, U Rochester｜⭐️ 

<u>http://arxiv.org/abs/2503.08689v1</u>  
<u>https://github.com/MAC-AutoML/QuoTA</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/199.jpg)
在长视频理解领域，传统的视觉冗余处理方法多依赖于基于注意力分布的视觉令牌修剪。然而，现有技术往往忽视了视觉令牌与查询指令之间的语义关联。为了解决这一问题，本文提出了QuoTA（Query-oriented Token Assignment），一种无需训练的模块化方法，旨在基于查询导向的帧级重要性评估来进行视觉令牌分配。QuoTA的设计使得视觉处理与特定任务需求相匹配，从而优化令牌预算的使用，同时保留语义相关内容。通过对现有大型视频语言模型（LVLMs）的扩展，QuoTA实现了在跨模态交互之前的视觉令牌一次性分配，显著提升了长视频理解的性能。实验结果表明，将QuoTA与LLaVA-Video-7B结合使用时，平均性能提升达3.2%，且在相同的视觉令牌预算下表现优于多种基线模型。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/200.jpg)
QuoTA的核心方法包括三个主要步骤：首先，动态帧采样器根据视频时长提取帧，以确保信息的有效性；其次，基于查询的分解，通过Chain-of-Thoughts（CoT）推理将查询转化为结构化问题，从而生成每帧的相关性评分；最后，视觉令牌分配器根据帧的重要性评分动态分配令牌数量。具体来说，方法分为以下几个部分：(1) 动态帧采样：根据视频时长动态调整采样帧的数量，确保长视频中关键信息的捕获；(2) CoT驱动的查询分解：将查询转化为具体问题，提升帧评分的准确性；(3) 动态视觉令牌分配：根据每帧的评分和预设的令牌预算，采用双线性插值、自适应池化或动态令牌合并等策略进行令牌分配，优化计算效率和准确性。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/201.jpg)
在实验部分，QuoTA在多个长视频理解基准上进行了验证，包括Video-MME、MLVU和LongVideoBench等。通过与LLaVA-Video-7B和LLaVA-OneVision-7B结合，QuoTA展现了优越的性能，尤其在处理长视频时，能够有效减少信息冗余，提升模型的理解能力。实验结果显示，QuoTA在Video-MME和VNBench等基准上取得显著提升，特别是在长视频（30-60分钟）场景下，性能提升更为明显。此外，通过对不同组件的消融实验，验证了动态帧采样和查询导向评分策略在提升模型性能方面的重要性。总体而言，QuoTA的实施证明了其在长视频任务中的有效性和灵活性。



### 通俗易懂
QuoTA方法的核心在于如何更聪明地处理视频中的信息。首先，我们从视频中选取一些关键帧，而不是每一帧都处理，这样可以减少冗余信息。接着，我们通过一种叫做Chain-of-Thoughts的方式，把用户的问题拆解成更具体的子问题，这样可以更准确地评估每一帧的重要性。最后，根据每一帧的重要性评分，我们动态分配视觉令牌，也就是处理信息的单位。我们可以用三种方法来调整这些令牌：简单的插值方法、自适应的池化方法，或者是根据帧之间的相似性来合并令牌。这种方法不仅提高了处理的效率，还能确保我们关注到视频中最重要的部分，从而更好地理解视频内容。 

## HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding 
2025-03-11｜UCF, Microsoft｜CVPR 2025｜⭐️ 

<u>http://arxiv.org/abs/2503.08585v1</u>  
<u>https://sacrcv.github.io/HierarQ-website/</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/226.jpg)
在视频理解领域，尽管多模态大型语言模型（MLLMs）取得了显著进展，但在处理中长视频时依然面临挑战，尤其是在帧和上下文长度方面的限制。为了解决这些问题，研究团队提出了HierarQ，这是一种任务感知的分层查询变换器框架。该框架通过顺序处理视频帧，避免了对帧采样的依赖，同时克服了LLM的上下文长度限制。HierarQ引入了一种轻量级的双流语言引导特征调制器，能够动态调整任务相关性，从而实现更全面的视频理解。通过在10个视频基准上的广泛评估，HierarQ展示了其在视频理解、问答和字幕生成任务中的卓越性能，证明了其强大的鲁棒性和效率。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/227.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/228.jpg)
HierarQ的核心在于其任务感知的处理机制，采用双流特征调制器来提升任务相关性。具体方法包括三个主要部分： 

1. **任务感知视觉特征提取**：该模型以自回归的方式顺序处理视频帧，使用预训练的视觉编码器提取视觉特征，并通过双流调制器增强与任务相关的特征。 
2. **分层查询变换器（HierarQ）**：通过短期和长期记忆银行，HierarQ有效地连接帧级别的实体细节与更广泛的场景上下文，模拟人类的认知处理方式。 
3. **文本解码**：将HierarQ的输出通过全连接层与语言模型结合，生成最终的文本输出，确保在处理长时间视频时有效管理上下文信息。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/229.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/230.jpg)
为验证HierarQ的有效性，研究团队在多个任务上进行了评估，包括中长视频理解、视频问答和视频字幕生成。实验使用了多个基准数据集，如LVU、Breakfast和COIN等，评估指标包括准确率和得分。结果显示，HierarQ在大多数基准上均表现出色，尤其在内容理解和问答任务中超越了现有最优模型。此外，研究还探讨了短期和长期记忆银行的设计对性能的影响，表明结合两者能够显著提升视频理解的准确性和鲁棒性。



### 通俗易懂
HierarQ的工作原理可以简单理解为一个智能的视频分析工具，它通过两个主要部分来处理视频信息。首先，它会逐帧分析视频，提取出每一帧中重要的视觉信息，比如人、物体等。然后，它会通过双流特征调制器来区分这些信息的重要性，确保在回答问题或生成字幕时，重点关注那些与任务最相关的内容。接着，HierarQ还会利用短期和长期记忆，分别储存即时的细节和更广泛的场景上下文，从而帮助模型理解视频的整体脉络。最后，经过这些处理后，HierarQ会将分析结果转换成自然语言，提供给用户一个清晰的回答或描述。这样，用户不仅能快速获取信息，还能更深入地理解视频内容。 
## RAG-Adapter: A Plug-and-Play RAG-enhanced Framework for Long Video Understanding 
2025-03-11｜NUDT, Hunan U, CSUFT, CCNU｜🟡 

<u>http://arxiv.org/abs/2503.08576v1</u>  
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/231.jpg)
在视频理解领域，多模态大语言模型（MLLMs）正在迅速发展，尤其是在处理长视频的能力方面。当前的评估方法，如Video-MME和MLVU，采用均匀帧抽样进行测试，这导致信息丢失，影响了模型性能的真实表现。为了解决这一问题，本文提出了一种名为RAG-Adapter的框架，它通过检索与问题最相关的帧来减少信息损失，从而更准确地评估MLLMs在长视频理解方面的能力。此外，研究中引入了分组监督对比学习（GCL）方法，以进一步提升RAG-Adapter的抽样效果。通过构建MMAT数据集并对多个基准进行测试，结果表明RAG-Adapter的抽样方法在准确性上始终优于均匀抽样，显著提高了模型在长视频理解任务中的表现。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/232.jpg)
RAG-Adapter的主要流程包括几个关键步骤。首先，在视频预处理阶段，从测试视频中以每秒一帧的速度抽取帧，并使用CLIP-L/14图像编码器将其转化为图像嵌入。接着，使用CogVLM2模型为每帧生成相应的字幕，并将其编码为文本嵌入。所有的图像和文本嵌入被存储在FramesDB和CaptionsDB数据库中。用户提出问题后，RAG-Adapter将问题进行编码，并与数据库中的帧和字幕进行匹配，检索出与问题最相关的TopK帧和TopN字幕。为了提高检索效果，RAG-Adapter还引入了双重重排序模块，通过最大边际相关性（MMR）算法优化检索结果，确保所选帧的多样性和相关性。此外，为了优化编码器的嵌入空间，使用了分组监督对比学习（GCL）进行微调，以提升RAG-Adapter在视频理解任务中的表现。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/233.jpg)
在实验部分，本文选择了Video-MME和MLVU等常用的长视频理解基准进行评估，此外还包括了Perception Test和EgoSchema等较短的基准。通过对90个视频样本的测试，评估了RAG-Adapter在不同视频长度和内容领域中的表现。实验结果表明，与均匀抽样相比，RAG-Adapter的抽样方法在所有基准上都显著提高了模型的准确度。例如，在知识、电影与电视、艺术表现等领域，GPT-4 Turbo模型的准确率提高了超过10%。同时，NIF指标的计算显示，RAG-Adapter能够有效识别出回答问题所需的最少帧数，进一步验证了其在长视频理解中的有效性和优越性。通过对比分析，RAG-Adapter展现了其在不同模型和基准上的广泛适用性和良好的性能提升。



### 通俗易懂
RAG-Adapter是一种增强视频理解模型的工具，主要通过选择与问题相关的帧来提高准确性。首先，它会从视频中每秒抽取一帧，然后将这些帧转换为计算机可以理解的数字格式。接下来，RAG-Adapter还会为每一帧生成描述性字幕。用户提出问题后，RAG-Adapter会将问题与帧和字幕进行匹配，找出最相关的内容。为了确保所选择的帧既相关又多样化，RAG-Adapter使用了一种叫做最大边际相关性的方法。最后，为了让模型更好地理解视频内容，RAG-Adapter还会对其进行微调，以确保它能更准确地捕捉到视频中的关键信息。通过这种方式，RAG-Adapter帮助模型在处理长视频时，能够更好地理解和回答问题。 
## DIV-FF: Dynamic Image-Video Feature Fields For Environment Understanding in Egocentric Videos 
2025-03-11｜U Zaragoza｜🟡 

<u>http://arxiv.org/abs/2503.08344v1</u>  
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/234.jpg)
动态图像-视频特征场（DIV-FF）是一种新颖的框架，旨在提高对自我中心视频环境的理解，尤其是在动态场景中。自我中心视频提供了第一人称视角，能够捕捉到人与环境的复杂交互。传统方法往往忽视了这些动态交互，导致对场景的理解受到限制。DIV-FF通过将场景分解为持久、动态和基于行为的组件，并结合图像和视频语言特征，显著提升了环境理解的能力。该模型不仅能够精确进行物体分割和语义场景分解，还能在时间上保持一致的理解，特别是在动态变化的场景中，DIV-FF展现出其强大的潜力。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/235.jpg)
DIV-FF的设计包括三个主要的特征流：持久环境流、动态环境流和行为流。每个流都使用神经辐射场（NeRF）来捕获场景的几何和外观特征。具体方法如下：

1. **动态神经辐射场**：通过集成三种辐射场，分别处理持久环境、动态对象和行为者，动态环境流通过输入特定的帧代码来捕获动态变化。
2. **图像语言特征场**：利用CLIP模型提取图像特征，通过精确的对象掩模来增强特征的空间对齐性，确保关键交互区域的语义信息被有效捕捉。
3. **视频语言特征场**：结合视频语言模型，捕捉时间依赖的语义信息，主要用于理解环境中的潜在交互和可用行为。该模型通过对视频描述的对比学习，增强了对动态语义的理解。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/236.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/237.jpg)
在EPIC-Kitchens数据集上进行的实验中，DIV-FF展示了在动态物体分割和可用行为识别方面的显著改进。实验结果表明，DIV-FF在动态物体分割任务中实现了30.5%的平均交并比（mIoU），比现有基线方法提高了40.5%。此外，DIV-FF在可用行为分割任务中也取得了20.7%的mIoU，相较于传统方法提升了69.7%。这些结果证明了DIV-FF在处理复杂动态场景时的有效性，尤其是在需要理解多种交互和动态变化的情况下。通过综合图像和视频特征，DIV-FF能够更准确地捕捉环境的丰富语义信息，推动了自我中心视频理解的研究进展。



### 通俗易懂
DIV-FF方法可以想象成一个聪明的机器人，它在观察周围环境时，能够同时记住哪些东西是静止的，哪些是移动的，还能理解自己可以做什么。这个机器人有三个“眼睛”：一个看着周围的固定物体，另一个专注于动态的物体，比如正在移动的人的手或正在飞的球，最后一个则是用来理解这些物体之间的互动。它通过分析视频和图像中的信息，来学习如何在这个环境中行动，比如“我可以在桌子上切东西”或者“我需要到水槽去洗手”。通过这种方式，DIV-FF帮助机器人更好地理解复杂的场景和动态变化，从而做出更聪明的反应。 

