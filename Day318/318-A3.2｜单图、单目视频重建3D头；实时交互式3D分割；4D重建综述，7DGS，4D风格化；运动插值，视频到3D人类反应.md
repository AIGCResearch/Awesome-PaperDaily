# Topic: 3D/4D Reconstruction｜Head 

## High-Quality 3D Head Reconstruction from Any Single Portrait Image 
2025-03-11｜SJTU, Shanghai AI Lab｜⭐️🟡 

<u>http://arxiv.org/abs/2503.08516v1</u>  
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/164.jpg)
本研究提出了一种新颖的高保真3D人头重建方法，能够从单一的肖像图像中生成高质量的3D头部模型。该方法不受视角、表情或配饰的限制，克服了传统方法在生成过程中面临的身份和表情一致性问题。为此，研究团队构建了一个新的多视角数字人类数据集，涵盖了227个数字人像序列，包含多种表情和配饰，总计21,792帧图像。通过将身份和表情信息整合进多视角扩散过程，研究者们能够增强面部特征在不同视角下的一致性。这一创新方法在生成多视角视频并利用其进行3D建模方面表现出色，尤其在侧面视角和复杂配饰的情况下，仍能保持高度的身份和表情一致性。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/165.jpg)
该方法的核心分为两个阶段：首先生成围绕输入肖像的多视角视频，然后利用这些视频重建3D头部模型。具体步骤如下：

1. **数据集构建**：创建一个多样化的数字人头数据集，包含不同的发型、肤色、年龄、性别、配饰和表情，确保在训练时具有高现实感。
2. **多视角生成**：采用稳定视频扩散（SVD）模型进行多视角图像生成。该模型经过大量视频数据预训练，能够高效捕捉时空一致性。
3. **3D重建**：使用3D高斯点云技术进行3D建模。通过对生成的多视角图像进行优化，快速而高效地重建出高质量的3D人头模型。
4. **身份和表情一致性增强**：在生成过程中引入身份感知指导，利用ArcFace网络提取面部特征，从而确保在不同视角下的身份一致性。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/166.jpg)
在实验部分，研究者们评估了所提出方法的有效性与其他现有技术的比较。通过构建一个包含新面孔和表情的测试数据集，选取10名受试者进行评估，每名受试者展示5种表情。研究团队使用多种评价指标，如LPIPS、PSNR、SSIM和身份保持度，来衡量生成图像的质量。实验结果表明，该方法在所有评估指标上均优于其他方法，特别是在处理极端表情、配饰和非正面视角时，能够保持高一致性和细节保真度。此外，研究还进行了消融实验，以验证身份和表情信息对模型性能的影响，结果显示，结合身份损失和表达增强的模型取得了最佳表现。



### 通俗易懂
在这项研究中，科学家们开发了一种从单张人脸照片生成3D头像的新方法。首先，他们创建了一个包含各种不同发型、肤色和配饰的照片库，以便让计算机学习如何更好地理解和重建人脸。接下来，他们使用一个叫做稳定视频扩散的技术，这种技术可以生成周围多个视角的头像视频。最后，利用这些视频，计算机能够快速地生成出一个高质量的3D人头模型。为了确保不同角度下的人脸看起来一致，研究者还加入了一些特别的算法来帮助计算机识别和保持每个人的独特特征。通过这些步骤，研究者们成功地让计算机能够在各种复杂情况下生成真实感十足的3D人头模型。 
## HRAvatar: High-Quality and Relightable Gaussian Head Avatar 
2025-03-11｜THU-SIGS, IDEA｜⭐️ 

<u>http://arxiv.org/abs/2503.08224v1</u>  
<u>https://eastbeanzhang.github.io/HRAvatar</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/170.jpg)
HRAvatar是一种新颖的3D高质量可动画头部虚拟形象重建方法，旨在通过单目视频输入生成可实时渲染的3D头像。该方法不仅在面部表情和头部姿势的捕捉上表现出色，而且能在不同光照条件下实现真实的重光照效果。传统的3D头像重建方法往往受到单视角输入信息有限的制约，导致面部跟踪不准确和形变灵活性不足，影响了最终的重建质量。HRAvatar通过引入3D高斯点和可学习的变形模型，解决了这些问题，从而在保持高效性的同时，提升了重建的真实感和细节表现。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/171.jpg)
HRAvatar的方法主要包括三个核心部分：精确的表情跟踪、几何变形建模和外观建模。首先，精确表情跟踪通过一个端到端的表达编码器来提取面部表情参数，这种方法能够减少传统面部跟踪的误差，提高重建质量。其次，在几何变形建模中，HRAvatar利用可学习的线性混合形状和混合皮肤技术，使得高斯点能够灵活地从标准形态变换到不同的姿态，从而适应个体的面部特征。最后，外观建模采用物理基础的阴影模型，通过分解头部外观为反射率、粗糙度等物理属性，实现了在未知光照条件下的真实重光照效果。这一系列方法保证了HRAvatar在实时重建和动画表现上的优越性。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/172.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/173.jpg)
在实验部分，HRAvatar的性能通过与多种先进方法的比较来验证。使用INSTA、HDTF和自捕获数据集进行测试，结果显示HRAvatar在PSNR、MAE、SSIM和LPIPS等多个指标上均优于其他方法。定量结果表明，HRAvatar不仅在重建质量上表现出色，还在动画渲染速度上达到了155FPS，确保了实时应用的可行性。此外，实验还展示了HRAvatar在复杂表情和姿势下的稳定性与细节再现能力，尤其在面部细节如眼睛、嘴巴和头发等方面，HRAvatar的重建效果明显优于基线方法。最终，实验结果验证了HRAvatar在高保真度和可重光照3D头像重建中的有效性。



### 通俗易懂
HRAvatar的工作原理可以简单理解为三个步骤。首先，HRAvatar通过一个智能程序来追踪和识别面部表情，这个程序能够准确捕捉到你脸上的每一个细微变化。接下来，HRAvatar会使用一种灵活的技术来调整头像的形状，使其能够完美适应每个人的独特面部特征。最后，HRAvatar会模拟真实的光照效果，让头像在不同的光线下看起来依然自然。这种方法的好处是，它不仅能快速生成高质量的3D头像，还能在不同环境下保持真实感，让你的虚拟形象在游戏、视频会议等场景中栩栩如生。
# Topic: 3D Segmentation
## WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images 
2025-03-11｜XMU, NUS｜⭐️ 

<u>http://arxiv.org/abs/2503.08407v1</u>  
### 概述 

WildSeg3D是一种新颖的实时交互式3D分割方法，旨在从2D图像中高效分割任意3D对象。传统的3D分割模型往往需要大量特定场景的训练，限制了它们在实际应用中的灵活性和速度。WildSeg3D通过引入动态全局对齐（DGA）技术，解决了多视角图像中3D对齐误差累积的问题，从而提高了分割的准确性。该方法结合了基于2D图像的分割模型（如Segment Anything Model, SAM），能够在不需要场景特定训练的情况下，快速实现3D对象的准确分割。WildSeg3D不仅在准确性上达到了当前最先进水平，还在处理速度上实现了显著提升，处理速度比现有模型快40倍。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/167.jpg)
WildSeg3D的工作流程分为三个主要阶段：2D掩膜预处理、3D点动态全局对齐和多视图组映射。首先，在2D掩膜预处理阶段，利用SAM2从多视角图像中生成目标对象的掩膜，并将其存储在掩膜缓存中，以便后续快速访问。接下来，3D点动态全局对齐阶段应用DGA技术，针对不同视角中的难以匹配的点进行动态权重调整，从而改善3D点图的对齐精度。最后，在多视图组映射阶段，系统根据用户输入从掩膜缓存中检索相关掩膜，并将其转换为统一的3D坐标系统，实现实时的3D分割。通过这些步骤，WildSeg3D有效地减少了背景干扰，提高了分割的准确性和实时性。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/168.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/169.jpg)
为了验证WildSeg3D的有效性，研究团队在多个基准数据集上进行了广泛的实验，包括NVOS和SPIn-NeRF等。实验结果表明，WildSeg3D在分割准确性（mIoU和mAcc）方面超越了现有的最先进方法，并且在处理速度上表现出色，完成3D场景重建的时间显著低于传统模型。具体而言，WildSeg3D在30秒内完成场景重建，而最快的对比模型SA3D则需要780秒。此外，WildSeg3D在实时交互分割中的响应时间也仅为5-20毫秒，展示了其在复杂场景中的强大适应能力和高效性。这些结果表明，WildSeg3D不仅在准确性上具备竞争力，同时也在实际应用中表现出色。



### 通俗易懂
WildSeg3D的工作流程可以简单理解为三个步骤。首先，它从不同角度的图片中提取出物体的轮廓，并把这些轮廓存储起来，以便快速使用。接着，系统会对这些轮廓进行调整，确保在不同视角下的物体能够准确对齐，这样就能避免因为背景混乱而导致的错误。最后，当用户需要分割某个物体时，系统会迅速从存储的轮廓中找到相关信息，并将其转化为3D空间中的准确位置。这一系列步骤不仅让分割过程更加快速，还保证了高准确性，能够在各种环境下灵活应用。 
# Topic: 4D Reconstruction｜Survey, 7DGS, Stylization,  

## Dynamic Scene Reconstruction: Recent Advance in Real-time Rendering and Streaming 
2025-03-11｜SEU, PKU｜⭐️🟡 

<u>http://arxiv.org/abs/2503.08166v1</u>  
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/181.jpg)
动态场景重建是计算机视觉和图形学中的一项重要而复杂的任务，其目标是从2D图像中表示和渲染动态场景。近年来，基于神经辐射场（NeRF）和3D高斯喷溅（3D Gaussian Splatting）的重建方法取得了显著进展。本文综述了动态场景表示和渲染的发展历程，特别关注这些新兴技术的演变和进展。通过系统性地总结现有方法，分类核心原理，编制相关数据集，比较不同方法在基准测试上的表现，本文探讨了这一快速发展的领域所面临的挑战和未来研究方向。总计回顾了170多篇相关文献，为该领域的前沿技术提供了广泛的视角。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/182.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/183.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/184.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/185.jpg)
本研究的方法主要集中在两种前沿技术：神经辐射场（NeRF）和3D高斯喷溅（3D-GS）。NeRF通过训练多层感知机（MLP）来隐式表示场景的连续五维函数，从而实现新视角的渲染。其核心包括以下几个方面：

1. **位置编码**：为输入位置进行傅里叶特征映射，以帮助网络学习复杂的空间模式。
2. **层次化采样**：通过训练两个网络（粗网络和细网络），在不同分辨率下预测场景的密度和颜色，从而在渲染质量和采样数量之间取得平衡。
3. **3D高斯喷溅**：该方法通过将场景表示为高斯分布，利用其物理特性实现高效的实时渲染，减少内存和计算需求。每个3D高斯具有中心位置、协方差矩阵、幅度和颜色等可学习属性。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/186.jpg)
在实验部分，本文评估了不同动态场景重建方法在多种数据集上的性能。通过对比神经辐射场和3D高斯喷溅方法的重建效果，分析了各自的优缺点。实验结果显示，基于NeRF的方法在捕捉细节和重建静态场景方面表现优异，但在动态场景中仍存在训练和渲染效率的问题。而3D-GS方法则在实时渲染和处理复杂场景时展现出更高的效率和灵活性。此外，本文还探讨了不同方法在动态场景重建中的应用潜力，强调了结合深度学习和传统图形学技术的重要性，以推动该领域的发展。



### 通俗易懂
在这项研究中，作者主要使用了两种新技术来处理动态场景的重建。首先是神经辐射场（NeRF），这是一种利用神经网络来理解和重建场景的方法。它通过将场景的每个点看作一个小的光源，来生成不同视角下的图像。为了让网络更好地学习场景的细节，作者引入了位置编码和分层采样的技巧。第二种方法是3D高斯喷溅，它通过将场景中的物体表示为一组高斯形状，来实现快速和高质量的渲染。这种方法更高效，因为它减少了对计算资源的需求，并且能够处理复杂的动态场景。总的来说，这项研究展示了如何将这些先进技术结合起来，以更好地理解和表现我们周围的动态世界。 
## 7DGS: Unified Spatial-Temporal-Angular Gaussian Splatting 
2025-03-11｜UII｜⭐️⭐️ 

<u>http://arxiv.org/abs/2503.07946v1</u>  
<u>https://gaozhongpai.github.io/7dgs/</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/187.jpg)
在计算机图形学中，实时渲染动态场景与视角依赖效果一直是一个重大挑战。尽管最近的研究在处理动态场景（如4DGS）和视角依赖效果（如6DGS）方面取得了一定进展，但尚无统一的方法能够同时满足这些要求并保持实时性能。为此，本文提出了一种新的框架——7D高斯点云（7DGS），通过将场景元素表示为七维高斯分布，涵盖空间（3D）、时间（1D）和视角方向（3D）。该方法的核心创新在于高效的条件切片机制，它将7D高斯转化为视角和时间条件下的3D高斯，从而实现与现有3D高斯点云管道的兼容，并支持联合优化。实验结果表明，7DGS在处理复杂动态场景时，PSNR提升可达7.36 dB，并且在实时渲染速度上也表现出色，达到401 FPS。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/188.jpg)
7DGS方法的设计基于一个七维高斯模型，该模型能够同时编码空间、时间和视角信息。具体而言，方法包括以下几个关键步骤：  

1. **高维表示**：每个场景元素被表示为一个7D高斯分布，包含空间位置、时间坐标和方向信息，这种高维表示能够捕捉几何、动态和外观之间的相互关系。  
2. **条件切片机制**：通过将7D高斯分布条件化为时间和方向的函数，生成一个适合实时渲染的3D高斯分布。这一过程确保了渲染的高效性与保真度。  
3. **自适应高斯细化**：为了解决动态场景中的复杂变形问题，方法采用自适应细化技术，通过神经网络预测残差动态调整高斯参数，以提高模型的准确性。  
4. **优化与渲染管道**：优化策略扩展了现有的3D高斯点云框架，确保了在新框架下的高效渲染，同时通过切片机制将7D表示转换为可渲染的3D格式。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/189.jpg)
为了验证7DGS的有效性，作者在三个不同的数据集上进行了广泛的实验：D-NeRF、Technicolor和自定义的7DGS-PBR数据集。实验结果显示，7DGS在多个评价指标上均优于现有方法，尤其是在处理复杂视角依赖效果的场景中，平均PSNR提升达4.71 dB，且使用的高斯点数量仅为4DGS的15.3%。此外，7DGS在动态场景渲染中的FPS表现也显著，达到401 FPS，显示出其在实时渲染中的潜力。通过与4DGS的比较，7DGS在多个场景中展现了更少的伪影和更高的几何重建精度，尤其在复杂光照交互的场景中，7DGS能够更好地捕捉几何和动态之间的关系。



### 通俗易懂
在7DGS方法中，研究者们开发了一种新型的高斯模型，用于更好地渲染动态场景。想象一下，我们要绘制一个运动中的物体，比如跳动的心脏或飘动的云彩。7DGS将这些物体的形状、运动和光线效果结合在一起，形成一个七维的高斯表示。这个表示不仅包括物体的空间位置，还包括它在时间上的变化和从哪个角度看起来好看。为了让渲染更快，7DGS使用了一种叫做“条件切片”的技术，可以根据时间和视角来调整这些高斯形状，让它们在不同情况下都能表现得很好。此外，7DGS还引入了一个自适应细化过程，确保在物体变形时，渲染效果依然准确。这种方法使得在复杂场景中实时渲染变得更加高效和真实。 
## GAS-NeRF: Geometry-Aware Stylization of Dynamic Radiance Fields 
2025-03-11｜TU Munich, MCML, Technion, Nvidia｜⭐️ 

<u>http://arxiv.org/abs/2503.08483v1</u>  
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/138.jpg)
本文提出了一种新颖的方法GAS-NeRF，旨在为动态场景中的辐射场实现风格化处理。现有的三维风格化技术多集中于静态场景，缺乏对动态场景中几何特征的充分考虑，致使风格化效果往往不够连贯。GAS-NeRF通过结合几何转移与外观风格化，使得风格化过程更加物理可信，保证了风格图像的颜色与深度信息能够有效地与动态场景的几何特征相匹配。该方法的创新之处在于首次实现了在动态辐射场中进行几何转移，提升了动态场景的整体视觉质量。实验结果显示，该方法在合成和真实数据集上均显著提高了风格化的效果，展现出良好的实用性和适应性。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/139.jpg)
GAS-NeRF的方法论主要分为几个步骤。首先，方法利用Hexplanes结构对输入的多视角视频进行训练，生成一个高质量的光场。接着，针对给定的风格图像及其深度图，方法分两步进行风格化处理：第一步是几何转移，第二步是外观转移。在几何转移阶段，使用预训练的辐射场对风格图像的几何特征进行调整，以确保生成的场景几何与风格图像相匹配。接着，使用最近邻特征匹配（NNFM）损失函数来优化几何转移的质量，并结合总变差损失与内容损失，确保细节的保留。外观转移阶段则是通过调整颜色来匹配风格图像的色彩特征，最终生成一个既保留几何特征又具备风格化外观的动态辐射场。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/140.jpg)
在实验部分，GAS-NeRF的方法在两个不同的数据集上进行了评估：真实世界的Plenoptic视频数据集和合成的D-NeRF数据集。与多种基线方法进行比较，包括ARF*和Ref-NPR*等，结果表明GAS-NeRF在深度图和颜色转移方面均取得了显著的提升。此外，采用LPIPS指标对生成图像的相似度进行了量化评估，结果显示GAS-NeRF在深度图的生成质量上优于其他方法。用户研究也表明，参与者对GAS-NeRF生成的风格化效果给予了更高的偏好度，进一步验证了该方法的有效性和优越性。通过这些实验，GAS-NeRF展示了其在动态场景风格化中的潜力和应用前景。



### 通俗易懂
GAS-NeRF的工作原理可以简单理解为一个两步走的过程。首先，它从一段视频中提取出场景的三维信息，然后根据一个风格图像来改变这个场景的外观。第一步是调整场景的形状和结构，确保它看起来与风格图像中的几何特征一致。比如，如果风格图像中有很多圆形的元素，GAS-NeRF会将场景中的相关部分也调整为圆形。第二步则是改变场景的颜色和纹理，使其看起来更像风格图像。这种方法的巧妙之处在于，它不仅仅是改变颜色，还考虑了形状的变化，这样最终生成的图像就更加自然和真实。通过这样的方式，GAS-NeRF能够在动态场景中实现更为协调和美观的风格化效果。 
# Topic: Motion Generation｜In-Betweening, Video to Human Reaction
## Towards Synthesized and Editable Motion In-Betweening Through Part-Wise Phase Representation 
2025-03-11｜FDU, Shanghai AI Lab, SJTU, WHU, HKU｜⭐️🟡 

<u>http://arxiv.org/abs/2503.08180v1</u>  
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/190.jpg)
本研究提出了一种新颖的框架，旨在实现对运动插值的精细化控制，特别是在计算机动画和游戏领域中。传统方法通常以整体身体运动为基础来编码运动风格，忽视了对各个身体部位独立风格的表现。这种方法的局限性在于，无法灵活调整特定肢体的运动风格。为了解决这一问题，我们的框架采用了身体部位级别的运动风格建模，增强了插值运动的多样性和可控性。通过引入周期性自编码器，我们能够自动提取每个身体部位的运动相位，从而实现对运动的细致调控。这种方法不仅提高了动画的表现力，还确保了整体运动的一致性，使得动画能够更真实地反映目标风格。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/191.jpg)
本框架的核心包括三个主要组件：身体部位相位自编码器（BP Phase Autoencoder）、身体部位专家混合模型（BPMoE）和运动采样器（Motion Sampler）。具体方法如下：

1. **BP Phase Autoencoder**：该组件负责从运动数据中提取各个身体部位的相位信息，形成局部相位流形。通过对运动数据进行编码，生成每个身体部位的相位向量，从而实现对运动动态的细致建模。
2. **BPMoE**：该模型通过多个专家网络实现对不同生物力学模式的动态融合。它根据当前状态和控制信号预测下一个状态，确保局部运动的精准性与全局协调性。
3. **Motion Sampler**：作为参数控制器，运动采样器根据当前和目标状态生成控制信号，从而实现时间和相位的一致性，确保运动的流畅过渡。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/192.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/193.jpg)
我们在100STYLE数据集上进行了全面的实验，评估了所提框架的有效性。实验设置包括对比当前主流方法，如CVAE、RSMT和PhaseMIB，分析它们在重建精度和运动自然性方面的表现。通过对不同风格的运动进行插值，我们考察了模型在长序列生成中的表现，尤其关注运动的多样性和可控性。实验结果表明，我们的方法在保持运动一致性的同时，能够实现对肢体运动风格的精确调整，显著优于现有技术。我们还进行了消融实验，验证了基于身体部位相位的风格编码在运动质量提升中的重要性，进一步证明了框架的有效性和灵活性。



### 通俗易懂
我们的研究方法可以简单理解为一种更聪明的动画制作工具。首先，我们将身体分成不同的部分，比如手臂和腿，然后为每个部分创建一个“运动相位”，就像是每个部分的运动节拍。接着，我们利用这些节拍来控制每个身体部分的运动，确保它们能够独立变化，同时又不会失去整体的协调感。这个过程就像是在编排一场舞蹈，每个舞者可以根据自己的节奏跳动，但整体的舞蹈看起来依然和谐。最后，我们的系统能够快速生成流畅的动画，让动画师在制作时可以轻松调整每个肢体的动作风格，达到更生动、更具表现力的效果。 
## AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models 
2025-03-11｜KAIST｜CVPR 2025｜⭐️🟡 

<u>http://arxiv.org/abs/2503.08417v1</u>  
<u>https://kwanyun.github.io/AnyMoLe_page/</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/194.jpg)
在动画制作中，关键帧插值是实现角色动作平滑过渡的重要技术。尽管近年来基于深度学习的运动插值方法取得了显著进展，但大多数方法仍然依赖于角色特定的数据集，这限制了其适用性。为了解决这一问题，本文提出了一种新颖的方法——AnyMoLe，利用视频扩散模型生成任意角色之间的过渡运动，而无需外部训练数据。该方法通过引入ICAdapt微调技术，缩小了真实世界与渲染动画之间的领域差距。此外，AnyMoLe采用了双阶段框架生成过程，以增强对上下文的理解，从而生成流畅且真实的过渡效果。这一创新使得AnyMoLe适用于更广泛的运动插值任务，显著降低了对数据的依赖性。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/195.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/196.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/197.jpg)
AnyMoLe的工作流程分为几个关键步骤。首先，利用给定的两秒上下文运动和目标关键帧，从多个视角渲染每一帧。其次，基于这些多视角渲染图像，微调视频扩散模型，并同时训练特定场景的关节估计器，以便有效地提取2D和3D特征。接下来，经过微调的视频扩散模型将生成插值视频，采用双阶段自回归方式：第一阶段生成稀疏帧以建立运动结构，第二阶段细化生成的帧，确保细节的完整性。最后，使用运动视频模仿技术，优化角色运动参数，使生成的运动与视频内容相一致。此过程中的关键在于使用上下文帧指导视频生成，以确保生成的运动符合预期的上下文信息。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/198.jpg)
在实验部分，AnyMoLe与多个基线方法进行了比较，包括ERD-QV、TST和SinMDM等。实验评估了旋转、位置和渲染图像的相似性，所有指标均与真实运动进行了对比。结果表明，AnyMoLe在所有评估指标上均优于其他方法，尤其是在运动自然性和风格一致性方面。此外，进行了消融实验，以验证ICAdapt和双阶段生成过程的重要性。实验结果显示，缺少ICAdapt会导致生成视频的风格不一致，而不使用细化阶段则会产生低帧率和相邻帧之间的显著跳跃。最终，AnyMoLe在多种角色的运动插值任务中均表现出色，展示了其在动画制作中的广泛应用潜力。



### 通俗易懂
AnyMoLe的工作原理可以简单理解为创建动画角色之间的平滑过渡。首先，研究人员从不同角度拍摄角色的运动，形成一段短视频。接着，他们使用这些视频来训练一个智能模型，使其能够理解角色的动作。这个模型不仅能看懂视频中的动作，还能生成新的动作画面。为了确保生成的动作看起来自然，研究人员设计了一个两步走的过程：第一步是先生成一些粗略的画面，第二步则是根据这些画面填补细节。最后，他们还加入了一种优化技术，确保生成的动作与原有的关键帧相匹配。这样的过程让动画制作变得更简单，因为它不再需要大量特定角色的数据，任何角色都可以轻松生成流畅的运动效果。 

## HERO: Human Reaction Generation from Videos 
2025-03-11｜USTC, HCNSC｜⭐️🟡 

<u>http://arxiv.org/abs/2503.08270v1</u>  
<u>https://jackyu6.github.io/HERO</u> 
### 概述 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/113.jpg)
在本研究中，我们提出了一种新的任务，即从RGB视频中生成3D人类反应。人类反应生成是交互式人工智能的重要研究领域，旨在使计算机能够合成自然且逼真的人类反应。以往的研究主要集中在基于人类运动序列合成反应运动，而忽视了情感对反应生成的影响。为了克服这些限制，我们提出了HERO框架，它能够从视频中提取交互意图，并利用这些信息生成反应。此外，我们还收集了ViMo数据集，包含了人类-人类、动物-人类和场景-人类等多种交互类别的数据，旨在为该领域的研究提供支持。通过大量实验，我们验证了HERO的有效性和优越性，展示了其在多种交互场景中的应用潜力。



### 方法 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/114.jpg)
HERO框架的核心在于其三个主要模块：视频编码器、运动VQ-VAE和反应生成模块。首先，视频编码器提取输入视频的视觉表示，包括局部视觉表示和全局视觉表示。局部表示为每一帧生成特征，而全局表示则通过对局部表示进行平均池化得到。其次，运动VQ-VAE学习运动与离散代码序列之间的映射，生成运动的离散表示。最后，反应生成模块提取交互意图并基于此生成反应。该模块采用全局-局部表示交叉注意力机制，动态调整局部表示的权重，以便更好地反映交互意图。同时，通过意图条件自注意力机制，将交互意图注入生成过程，确保生成的运动与输入视频内容相符。此外，利用动态信息的提取，进一步提高生成反应的质量和自然度。



### 实验 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/115.jpg) 
![](http://huyaoqi.tpddns.cn:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-03-13/116.jpg)
在实验部分，我们使用ViMo数据集进行训练和评估。数据集包含3500对视频-运动样本，涵盖了32个子类别的交互类型。我们将数据集划分为训练集和测试集，并使用多种评价指标对HERO模型进行评估，包括Frechet Inception Distance（FID）、多样性和多模态性。实验结果表明，HERO在FID和多样性指标上均优于现有的基线方法，显示出其生成反应的能力和多样性。同时，我们还进行了用户研究，收集参与者对生成运动质量和反应真实性的评分，结果表明HERO生成的反应在情感表达和运动自然度上均表现良好。此外，我们还进行了消融实验，验证了交互意图提取和动态信息利用对生成效果的贡献。



### 通俗易懂
HERO框架的工作原理可以简单理解为三个步骤。首先，系统会分析输入的视频，提取出每一帧的视觉信息，比如人物的动作和表情。接着，系统会将这些信息转换成一种简化的代码形式，方便后续处理。最后，系统会根据这些信息生成相应的人类反应，像是一个人看到另一个人走过来时的反应。这个过程不仅考虑了动作本身，还会根据视频中人物的情绪来调整反应，比如如果一个人微笑着走过来，另一个人可能会主动打招呼，而如果是愤怒的表情，反应可能就是退后。这样，HERO能够生成更加自然和真实的人类反应，使得计算机在与人类互动时更具人性化。 