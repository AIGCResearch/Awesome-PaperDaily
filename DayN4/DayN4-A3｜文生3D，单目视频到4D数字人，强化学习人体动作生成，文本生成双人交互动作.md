# Topic: 3D/4D Generation 
## Generating Surface for Text-to-3D using 2D Gaussian Splatting 
2025-10-08｜UESTC 

<u>http://arxiv.org/abs/2510.06967v1</u>  
### 概述 
![](https://img.blenet.top/file/paper-daily/aigc_research/38b80b1499494681a60235ec9eb19dbb.png)  
本文针对文本驱动的3D物体生成任务，提出了一种基于2D高斯点片（Gaussian Splatting）的新颖方法DirectGaussian。传统Text-to-3D方法多依赖多视角图像生成，但视角数量有限导致几何和纹理细节不足，且难以保证多视角间的一致性。DirectGaussian通过参数化3D物体表面为高斯点片（surfels），结合多视角法线和纹理先验，直接生成高质量的表面表示。为解决多视角几何一致性问题，方法引入360°曲率约束和表面收敛约束，显著提升了生成物体的细节丰富度和视角一致性。该框架不仅提高了生成效率，还兼顾了几何和纹理的高保真度，展示了在动画、虚拟现实和游戏内容生成等领域的广泛应用潜力。

### 方法 
![](https://img.blenet.top/file/paper-daily/aigc_research/d7db249d461e41bdaf762225ea5e067d.png)  
DirectGaussian方法主要包括三个核心步骤：  

1. **高斯点片数据集构建**：基于Cap3D和Objaverse数据集，提取物体表面点云及对应的多视角图像和相机参数，构造与文本描述匹配的高斯点片数据集。  
2. **粗糙高斯点片生成模型**：采用多头注意力机制的编码器-解码器结构，利用预训练的多视角法线扩散模型和法线条件纹理扩散模型生成粗糙的高斯点片参数（位置、尺度、旋转、颜色和透明度），实现文本到表面的初步映射。  
3. **优化过程与约束引入**：通过2D高斯点片渲染策略，从四个固定视角渲染法线和纹理图像，定义包含RGB纹理损失、深度畸变损失和法线一致性损失的综合损失函数。为增强几何一致性，加入360°曲率约束，确保重叠视角间法线曲率一致；引入表面收敛约束，促使点片紧贴真实表面，减少视角间冗余和噪声。优化过程分阶段引入约束，保障生成稳定且细节丰富。

### 实验 
![](https://img.blenet.top/file/paper-daily/aigc_research/9af6ccbb2533431395832351c3f4d869.png) 
![](https://img.blenet.top/file/paper-daily/aigc_research/52c944d68bdf4120b9cb332391eaf701.png) 
![](https://img.blenet.top/file/paper-daily/aigc_research/88fc5b0b84974f1dbac2fc44f5453797.png)  
在多个文本描述下，DirectGaussian生成的3D模型在视觉质量和文本一致性方面均优于现有基于高斯点片及2.5D扩散的主流方法。定量上，采用CLIP评分评估文本与生成物体的相关度，DirectGaussian在多组测试中取得最高分。用户研究显示，29.8%的受访者更偏好该方法生成的模型，超过其他对比方法。消融实验验证了粗糙高斯点片生成模型和曲率约束对提升几何细节和视角一致性的关键作用。此外，方法在保证生成质量的同时，保持了较快的训练和生成速度，展示了良好的实用性和鲁棒性。多视角渲染结果显示，生成模型能在360°范围内保持几何和纹理的连贯性，显著减少了传统方法中的多面体“Janus”问题。

### 通俗易懂  
DirectGaussian的核心思想是把3D物体的表面用许多小的“高斯点片”来表示，这些点片就像小圆盘，每个都有自己的位置、方向和颜色。首先，系统根据输入的文字描述，利用训练好的模型生成一个粗略的点片集合，这些点片大致勾勒出物体的形状和纹理。接着，通过从不同角度渲染这些点片，计算它们与真实图像之间的差距，并用这个差距来调整点片参数，使得生成的3D表面更准确。为了保证物体从各个角度看起来都一致且细节丰富，方法特别设计了两个约束：一个是360度的曲率约束，确保物体表面弯曲的地方在不同视角下保持一致；另一个是表面收敛约束，防止点片偏离真实表面。这样，最终生成的3D物体不仅符合文字描述，还拥有连贯的形状和细腻的纹理，适合用在动画和虚拟现实中。 
# Topic: 3D/4D Reconstruction｜Human
## MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis 
2025-10-08｜CUHK(SZ), FNii-Shenzhen, GBU, SZU, GPKL-FNI｜SIGGRAPH Asia 2025 

<u>http://arxiv.org/abs/2510.07190v1</u>  
<u>https://github.com/zyhbili/MV-Performer</u> 
### 概述 
![](https://img.blenet.top/file/paper-daily/aigc_research/c78463f3a68049708c3ce075e4829d53.png) 
![](https://img.blenet.top/file/paper-daily/aigc_research/b63585b367b84c80a063af8ae9608ae7.png)  
本文提出了MV-Performer，一种创新的视频扩散模型框架，专注于从单目视频输入生成360度同步的人体多视角4D新视图合成。当前视频扩散技术虽能隐式实现4D新视图合成，但多聚焦于摄像机轨迹重定向，难以覆盖全方位视角变化，且在人体动态场景中保持视角和时间一致性存在挑战。MV-Performer借助MVHumanNet数据集和预训练的WAN2.1视频扩散模型，结合摄像机依赖的法线图作为几何引导，有效缓解单目深度估计的不确定性，提升大视角变化下的合成准确性与一致性。该方法不仅实现了高质量的360度多视角视频生成，还提供了鲁棒的推理流程以应对野外单目视频中的深度估计误差，显著优于现有基线方法。实验覆盖多个数据集，验证了其在空间细节、感知质量及时间连贯性上的领先表现，推动了人类中心4D新视图合成技术的发展。

### 方法 
![](https://img.blenet.top/file/paper-daily/aigc_research/2531897e2a35471b8029cdc3b3302ef7.png)  
MV-Performer方法主要包括三大核心模块：  

1. **多视角视频扩散模型设计**：基于预训练WAN2.1视频扩散模型，模型通过引入多视角注意力机制融合参考视频和多视角几何条件，实现跨视角信息的同步和共享，保证生成视频在不同视角间的一致性。  
2. **摄像机依赖的法线图条件引入**：针对大视角变化中前后视角混淆问题，利用单目视频估计的深度和法线，通过点云投影生成视角相关的法线图，作为几何先验引导扩散模型，帮助区分可视与不可视区域，提升视角转换的准确性。  
3. **深度估计优化与推理流程**：鉴于单目深度估计存在漂移和误差，设计多阶段深度对齐和优化策略，结合最新深度估计技术，减少深度误差对几何条件的影响，确保推理阶段生成视频的质量和稳定性。此外，采用同步注意力模块强化多视角时间一致性，提升多视角视频的空间与时间连贯性。

### 实验 
![](https://img.blenet.top/file/paper-daily/aigc_research/52466cd7fcde45239524fce878a6e2e2.png) 
![](https://img.blenet.top/file/paper-daily/aigc_research/23573b4ad9534cf78fb6edf72ee25332.png)  
实验部分在MVHumanNet和DNA-Rendering两个多视角人体视频数据集上进行，涵盖定量和定性评测。通过与三种基线方法（TrajectoryCrafter、ReCamMaster及Champ）对比，MV-Performer在PSNR、SSIM、LPIPS、FID和FVD五项指标上均显著优于其他方法，特别是在时间一致性和视角一致性方面表现突出。消融实验验证了摄像机依赖法线图条件和同步注意力模块对性能的关键贡献，缺失任一模块都会导致生成质量和一致性的明显下降。此外，深度优化策略在野外单目视频测试中有效降低了深度误差带来的漂浮伪影，提升了模型的泛化能力。采样步数实验显示，25至50步采样在质量和计算成本间达到最佳平衡。最终，MV-Performer不仅在标准数据集上表现优异，还成功推广至野外单目视频，展现出良好的实用性和鲁棒性。

### 通俗易懂  
简单来说，MV-Performer就像一个非常聪明的视频“魔法师”，它能用你手机拍摄的单人视频，生成从各个角度看同一个人的同步视频，就好像你用多台摄像机同时拍摄一样。它的秘诀有三个：首先，它用一个强大的视频生成模型，这个模型能学习把一个角度的视频变成多个角度的视频。其次，它利用从视频里估计出来的“法线图”（可以想象成物体表面的方向指示图），告诉模型哪些地方是面向摄像机的，哪些是背对的，这样生成的视频角度转换更准确。最后，为了让生成的视频不出现闪烁或错位，它还特别设计了一个同步机制，让不同角度的视频在时间上保持一致。除此之外，它还会对视频里的深度信息进行优化，避免因估计误差导致的视频质量下降。总的来说，MV-Performer让普通单摄像机拍摄的视频，变成了多角度同步的4D视频，效果真实又连贯，非常适合做虚拟现实、电影特效等应用。 
# Topic: Motion Generation 
## No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts 
2025-10-08｜Florence 

<u>http://arxiv.org/abs/2510.06988v1</u>  
### 概述 
  
本文针对基于扩散模型的人体动作生成存在的适应性差、需依赖大量动作捕捉数据和重新训练的问题，提出了一种无需动作捕捉数据、仅利用文本提示进行后训练的强化学习微调框架。该方法通过预训练的文本-动作检索网络作为奖励信号，结合去噪扩散策略优化（DDPO），实现对预训练动作扩散模型的生成分布向目标领域迁移。此方式不仅避免了昂贵的动作数据采集和模型重训，还能在跨数据集和留一类动作的零样本设置下，提升动作生成的质量和多样性，同时保持对原始数据分布的性能。该框架灵活高效且保护隐私，为动作生成模型的快速适应与推广提供了新的解决方案。

### 方法 
![](https://img.blenet.top/file/paper-daily/aigc_research/8f6d5c8016d14643824e6a4240a227a7.png)  
本文方法主要包含三大核心组件：  

1. **强化学习策略优化（DDPO）**：将动作扩散模型的反向去噪过程视为马尔可夫决策过程（MDP），每一步的去噪动作对应策略动作。利用DDPO框架，以PPO为基础，结合重要性采样提升样本利用率，实现对扩散模型参数的稳定微调。  
2. **文本-动作检索奖励模型**：采用预训练的文本-动作检索网络评估生成动作与文本提示的语义匹配度，作为唯一的奖励信号。该模型通过计算文本和动作编码的余弦相似度，避免了对配对动作数据的依赖，确保训练过程中的数据隐私。  
3. **高效训练策略**：引入低秩适配（LoRA）技术，仅微调扩散模型中的少量参数，减少训练开销和过拟合风险。同时，采用DPM-Solver++加速采样，将采样步数大幅缩减，提高训练和推理效率。  

### 实验 
![](https://img.blenet.top/file/paper-daily/aigc_research/6e12b861bcd94c40a5d072060a9adb00.png) 
![](https://img.blenet.top/file/paper-daily/aigc_research/80c0a61015ab4980a23ca8e214504b31.png) 
![](https://img.blenet.top/file/paper-daily/aigc_research/26446acef0254dba92ddf59ba546edc0.png)  
在HumanML3D和KIT-ML两个公开数据集上，本文方法在跨数据集和留一类动作零样本适应任务中表现出显著优势。跨数据集实验显示，微调后模型在FID指标上提升15%-30%，文本-动作匹配准确率（R@1等）提升2%-5%，显著优于多种基线模型。留一类动作实验进一步验证了方法对未见动作类别的适应能力，微调模型甚至超越了完整训练模型的表现。用户研究中，参与者一致认为微调模型生成的动作在文本贴合度和真实感上更优。更重要的是，微调过程对原始训练数据分布无负面影响，甚至带来轻微性能提升，表明方法具备良好的泛化和正向迁移能力。

### 通俗易懂  
这项研究的核心是让已经训练好的动作生成模型“自我学习”，只用文字描述来改善自己，完全不用再去拍摄真实动作数据。具体做法是：首先，模型生成一段动作；然后，一个专门训练好的“评分器”会比较这段动作和文字描述的匹配程度，给出一个分数；接着，模型根据这个分数调整自己，努力生成更符合文字描述的动作。这个过程类似于老师给学生打分，学生根据反馈改进表现。为了让训练更快更稳定，研究者只调整模型里一小部分参数，并用更快的计算方法生成动作。这样，模型就能在不增加额外昂贵数据的情况下，快速适应新动作类型，生成更加自然、符合要求的动作，且还能保持原有动作的质量。整个过程既节省资源又保护隐私，非常适合实际应用。 
## Text2Interact: High-Fidelity and Diverse Text-to-Two-Person Interaction Generation 
2025-10-07｜Penn, HKU, Snap 

<u>http://arxiv.org/abs/2510.06504v1</u>  
### 概述 
![](https://img.blenet.top/file/paper-daily/aigc_research/6b2d113981214833bb4258663447f700.png)  
本文针对从文本生成高保真且多样化的两人交互动作序列的难题，提出了Text2Interact框架。该任务不仅要求单人动作逼真，还需实现两人之间时空上的精确耦合，且动作需与文本语义高度一致。现有方法受限于两人交互训练数据稀缺，难以覆盖丰富的交互细节；同时文本条件通常被简化为整体句子嵌入，忽略了词级别的细粒度信息，导致生成的动作与文本语义匹配度不足。为解决这些问题，作者设计了两大核心模块：InterCompose通过组合单人动作和大语言模型（LLM）生成的交互文本，合成多样且高质量的两人交互数据；InterActor则基于词级条件和自适应交互损失，细致地捕捉动作间的语义和时空耦合，实现精确的文本到动作映射。该框架在多个指标上优于现有方法，特别是在文本动作匹配度和动作多样性方面表现突出，并通过用户研究验证了其在真实场景下的泛化能力。  

### 方法 
![](https://img.blenet.top/file/paper-daily/aigc_research/e9700a2efdce43bb831dede90c2cf4a4.png)  
Text2Interact由两部分组成：  

1. **InterCompose：数据合成模块**  
   - 利用LLM对现有交互文本进行主题和细粒度标签分类，生成丰富的交互描述。  
   - 将两人交互文本拆分为两个角色的单人动作描述，基于预训练的单人动作生成模型生成对应动作。  
   - 训练条件扩散模型，生成第二个角色的反应动作，保证动作语义一致且时空协调。  
   - 设计双阶段过滤机制，采用对比学习编码器评估文本与动作匹配度，剔除低质样本，并通过距离度量筛选多样且合理的动作样本。  
2. **InterActor：细粒度交互生成模型**  
   - 引入词级别跨注意力机制，使每个动作时间步动态关注文本中对应的词汇，保持动作与文本的细节对齐。  
   - 设计交替的动作-动作交互模块，先对自身动作序列进行自注意，再对另一角色动作进行交叉注意，捕捉两人间的物理和时间依赖。  
   - 提出自适应交互损失，根据关节对的空间接近度加权，强化关键交互部位的动作一致性。  
该方法充分利用单人动作先验和细粒度文本信息，显著提升了两人交互动作的多样性和语义准确性。  

### 实验 
![](https://img.blenet.top/file/paper-daily/aigc_research/5270cca81a4141ebadbe3ce76dd29553.png) 
![](https://img.blenet.top/file/paper-daily/aigc_research/bbb7f97fee8a45d298360a922febeba8.png)  
实验基于InterHuman数据集，结合合成数据对模型进行训练和微调。采用多项指标评估，包括文本-动作匹配的R-Precision，动作质量的FID和多样性指标。结果显示，InterActor在文本匹配度（R-Precision）上显著优于现有最优方法，且在动作质量和多样性方面保持竞争力。定性对比表明，InterActor生成的动作更符合文本描述，且姿态更自然合理。通过用户研究，微调合成数据后模型在真实场景文本下表现出更强的泛化能力和用户偏好。进一步的消融实验验证了词级条件模块、自适应交互损失和合成数据微调对性能提升的贡献。合成数据的双重过滤策略有效保证了训练数据的语义一致性和多样性。整体实验充分证明了Text2Interact在提升两人交互动作生成的真实性、文本一致性和多样性方面的有效性。  

### 通俗易懂  
这个方法主要解决的是如何根据一段描述两个人互动的文字，生成两个人的动作视频。它分两步走：第一步，先用大语言模型写出两个人分别的动作描述，然后用已经训练好的单人动作模型，把这些描述变成单个人的动作。接着，训练一个“反应生成器”，让第二个人的动作和第一个人的动作配合得更自然。第二步，设计了一个特别的动作生成模型，它能在生成动作时，仔细“听”每个词的意思，而不是简单理解整句话，这样动作和文字的对应更细致。它还会让两个人的动作互相关注，保证他们的动作互动合理。为了确保生成的动作既真实又多样，还设计了一个过滤机制，筛掉不合适的动作。最后，模型在多项指标和用户评价中都表现很好，能生成既符合文字描述又自然流畅的两人互动动作。简单来说，就是先把复杂的两人动作拆成单人动作，再让模型学会两人如何配合，最终生成符合文字描述的高质量动作。 
