# fMRI引导图像重建，组合泛化能力，向量量化相关，流匹配与强化学习，生物启发扩散模型；个性化隐私保护，水印技术综述
# Topic: Image Generation
## HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion 
2025-10-03｜SUS Tech, SIAT, CAS 

<u>http://arxiv.org/abs/2510.03122v1</u>  
### 概述 
  
本研究针对从人类脑部fMRI信号重建复杂视觉场景图像的难题，提出了HAVIR模型。该模型灵感来源于视觉皮层的层级处理机制，将视觉信息分为结构信息和语义信息两条路径分别处理。现有方法在处理复杂场景时难以兼顾低级结构细节和高级语义准确性，主要因为自然场景中低级特征高度异质且高层语义交织混杂。HAVIR通过分离处理这两类信息，分别从空间处理区域提取结构特征，从语义处理区域提取语义嵌入，最后利用一个多模态扩散模型融合生成最终图像。该方法在多个评测指标上显著优于当前最先进技术，尤其在复杂场景的重建质量和语义还原方面表现突出，展示了神经科学与计算机视觉跨学科融合的潜力。

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/52.jpg)  
HAVIR由三大核心部分组成：  

1. **结构生成器**：将空间处理区的fMRI体素映射到潜在扩散空间的低维表示，捕获图像的空间布局和边缘信息。训练时结合均方误差和Sobel边缘损失，确保结构细节的准确还原。  
2. **语义提取器**：将语义处理区的fMRI体素转换为CLIP的图像及文本嵌入，利用对比学习和均方误差损失保证语义信息的准确映射。语义嵌入作为条件引导扩散模型的图像生成过程。  
3. **多模态扩散模型**：以结构生成器输出的潜在变量为基础，加入语义提取器提供的双模态CLIP嵌入，通过逐步去噪重建高质量图像。该过程实现了结构与语义的有效解耦和融合，保证图像既具备空间精度又语义丰富。模型采用个体化脑区掩膜，适应不同受试者的神经解剖差异，提升解码精度。

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/53.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/54.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/55.jpg)  
实验基于NSD大规模7T fMRI数据集，涵盖八名受试者观看COCO图像。采用个体化脑区掩膜划分空间与语义处理区。实验设计包括统一测试集和个体训练集，保证评估的公平性和泛化能力。重建结果在定性和定量上均优于五种最新方法，特别是在复杂场景的背景纹理、部分遮挡物体及小尺寸目标的表现上更为出色。多层次评测指标涵盖像素相关性、结构相似度、中级纹理特征和高级语义一致性，HAVIR在多数指标上取得最佳或次佳成绩。消融实验验证了结构先验和语义引导的互补性，缺一不可。此外，模型通过权重反向映射揭示了各脑区对结构和语义解码的贡献，体现了跨受试者的适应性和个性化解码策略。

### 通俗易懂  
想象我们看一幅复杂的图片时，大脑其实是在同时处理两方面的信息：一方面是图片的“骨架”——比如物体的形状、位置和边缘，这些是低级的结构信息；另一方面是图片的“意义”——比如物体是什么，它们之间的关系，这些是高级的语义信息。HAVIR模型就像大脑一样，把这两种信息分开处理。它先用一个“结构生成器”把大脑中负责空间感知的信号转成图像的轮廓和布局；再用一个“语义提取器”把负责理解意义的信号转成文字和图像的语义描述。然后，一个聪明的图像生成器把这两部分信息融合起来，最终画出既有清晰结构又符合语义的图像。这样做好比先画出草图，再给草图填充颜色和细节，既保证画面准确，也让内容丰富。通过这种分工合作，HAVIR能更好地还原我们脑中看到的复杂场景。 
## What Drives Compositional Generalization in Visual Generative Models? 
2025-10-03｜Freiburg, BCAI, Inria-ENS-CNR-PSL 

<u>http://arxiv.org/abs/2510.03075v2</u>  
### 概述 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/56.jpg)  
本文聚焦视觉生成模型中的组合泛化能力，即模型能否生成训练中未见过的已知概念的新组合。当前视觉生成模型虽能生成高质量图像和视频，但其组合泛化表现参差不齐。通过对比两种主流模型MaskGIT和DiT，发现它们在生成“非微笑金发男性”等新组合时表现迥异，揭示了组合泛化的关键驱动因素。研究将生成模型拆解为三个核心组件：编码器（Tokenizer）、生成模型和条件信号，围绕这三者展开系统实验，探讨设计选择如何正负影响组合泛化。核心结论指出，训练目标若作用于连续分布，且训练时条件信号提供完整的组成概念信息，则模型更易实现稳健的组合泛化。基于此，提出在MaskGIT的离散训练目标上增加辅助的连续型JEPA目标，有效提升其组合泛化能力，推动视觉生成模型向更具系统性和创造性的方向发展。

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/57.jpg)  
本研究设计了一套系统实验框架，逐步剖析影响视觉生成模型组合泛化的关键设计因素，具体方法包括：  

1. **模型拆解与设计变量控制**：将生成流程分解为Tokenizer（VAE与VQ-VAE）、生成模型（连续分布vs离散分布，扩散损失vs掩码损失）和条件信号（完整vs量化或不完整信息），通过逐一替换这些变量，观察组合泛化性能变化。  
2. **训练目标与输出空间分析**：重点比较连续输出空间（如DiT采用的扩散目标）与离散输出空间（MaskGIT的分类目标）对组合泛化的影响，验证连续目标更有利于组合泛化。  
3. **条件信号信息量实验**：通过对条件信号进行量化和丢弃部分信息，探测信息完整性对组合泛化的影响，发现完整精确信息是实现良好组合泛化的关键。  
4. **辅助连续目标引入**：针对离散模型MaskGIT，设计辅助的JEPA目标，利用连续中间表示的重建损失，促进模型学习更解耦的语义特征，从而提升组合泛化。  
5. **定量与机制分析**：通过线性探针评估生成内容的因素准确性，并利用机制解释技术测量注意力头的多义性及神经元的因果影响，揭示辅助目标带来的表示结构改善。

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/58.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/59.jpg)  
实验基于多个数据集（包括Shapes2D合成图像、CelebA人脸属性及视频数据集CLEVRER-Kubric）展开，采用条件生成任务训练模型在部分组合上，测试其对未见组合的生成能力。主要实验发现：  
- **Tokenizer类型无显著影响**：无论VAE还是VQ-VAE，DiT均能实现良好组合泛化，说明编码方式不是关键因素。  
- **训练目标与输出空间决定性**：连续输出空间（DiT、MAR、GIVT）模型在组合泛化上显著优于离散输出（MaskGIT），且掩码训练目标并非限制因素。  
- **条件信号完整性影响大**：量化或随机丢弃条件信息导致组合泛化性能大幅下降，强调训练时条件信息的完整性。  
- **辅助JEPA目标提升离散模型表现**：在MaskGIT中加入JEPA辅助目标后，组合泛化能力明显提升，且模型内部注意力头的多义性减少，神经元因果重叠降低，表明模型学习到了更解耦的语义因素。  
此外，研究还在真实驾驶视频和语言模型任务中验证了连续目标对组合泛化的正向作用，显示其跨模态的潜在适用性。

### 通俗易懂  
这项研究主要探讨为什么有些视觉生成模型能“聪明地”组合已学过的元素，创造出新颖的图像，而另一些却做不到。研究发现，关键在于模型学习的“语言”是连续的还是离散的。连续的“语言”就像我们说话时语调和语气的连贯变化，能更灵活地表达新意思；而离散的“语言”像是只能用固定词汇拼凑，限制了创造力。研究还发现，模型在训练时如果能获得完整、精确的提示信息，就更容易学会灵活组合这些元素。基于这些发现，研究者给一个表现不佳的模型加了一个辅助任务，让它在学习离散符号的同时，也学会理解连续的隐藏信息，这样模型就能更好地“理解”各个部分之间的关系，从而生成更多新颖的组合。简单来说，就是让模型既懂“拼字游戏”，又能“听懂语境”，这样它就能更聪明地创造新图像。 
# Topic: Image Generation｜Diffusion/Autoregress/VQ｜BBB10.6 
## Product-Quantised Image Representation for High-Quality Image Synthesis 
2025-10-03｜Uni HD 

<u>http://arxiv.org/abs/2510.03191v1</u>  
### 概述 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/60.jpg)  
本文提出了PQGAN，一种基于产品量化（Product Quantisation, PQ）的高质量图像合成潜空间表示方法。传统的向量量化（Vector Quantisation, VQ）在高维潜空间学习中存在训练信号稀疏、收敛缓慢和冗余度高的问题。PQGAN通过将潜向量划分为多个子空间，分别独立量化，形成一个组合式的虚拟码本，大幅提升了表示能力和训练效率。该方法不仅在图像重建质量上显著优于现有量化和连续潜空间技术，实现了PSNR从27dB提升至37dB，且在FID、LPIPS和CMMD等感知质量指标上提升高达96%。此外，PQGAN可无缝集成到预训练的扩散模型中，实现更高分辨率的生成或更快的生成速度，极大地提高了潜空间离散表示在图像生成中的实用性和扩展性。本文还深入分析了码本大小、嵌入维度和子空间划分的关系，揭示了PQ与VQ在维度扩展上的相反性能趋势，为超参数选择提供了理论指导。

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/61.jpg)  
PQGAN基于VQGAN架构，将传统的单一向量量化模块替换为产品量化模块，具体方法包括：  

1. **向量量化基础**：传统VQ通过一个码本将每个潜向量替换为最近的码字，维度越高训练越困难，码本学习信号稀疏。  
2. **产品量化设计**：将潜向量划分为S个互不重叠的子空间，每个子空间单独训练一个小码本。每个潜向量由多个子空间码字组合而成，形成了一个虚拟的组合码本，大小为K^S，大幅增加表示容量。  
3. **训练目标**：继承VQGAN的重建损失、码本承诺损失及对抗损失，确保量化潜空间既准确又稳定。  
4. **潜空间适配**：针对扩散模型，修改U-Net的输入输出层以匹配高维量化潜向量，采用两阶段训练策略，先冻结大部分参数只训练适配层，再全模型微调，保证生成质量和计算效率。  
该方法有效解决了高维潜空间的训练难题，同时保持了低空间分辨率带来的计算优势，适合扩散模型等生成任务。

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/62.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/63.jpg)  
实验部分系统评估了PQGAN在重建质量、码本利用率及生成性能上的表现。首先，通过调节潜向量维度d、子空间数S和码本大小K，发现PQ与传统VQ在维度扩展上的表现截然不同：VQ维度增大性能下降，而PQ维度增大性能提升，且子空间数达到d/2时性能趋于饱和。码本利用率指标显示PQ在高维量化下依然能保持高效且均匀的码字使用。其次，PQGAN在ImageNet 256×256数据集上与多种最先进的量化和连续潜空间方法对比，PQGAN以128维、512码本的配置实现了37.4dB的PSNR，FID低至0.036，显著领先。迁移到FFHQ和LSUN数据集时，PQGAN同样表现出优异的泛化能力。最后，将PQGAN潜空间集成到Stable Diffusion 2.1中，展示了在保持生成速度和内存成本不变的情况下，分辨率提升一倍或生成速度提升四倍的可能，验证了PQ潜空间的实用价值。

### 通俗易懂  
PQGAN的核心想法是把一个大“拼图”分成很多小块，分别给每块找最合适的“图案”，然后把这些小块的图案组合起来，形成一个超级大的“图案库”。传统方法是直接用一个大“图案库”来匹配整个拼图，随着拼图变大，找到合适图案变得很难，训练起来也慢。PQGAN把拼图拆成小块，每块单独找图案，训练时信号更丰富，学习更快也更准确。这样，虽然每个小块的“图案库”不大，但组合起来的整体“图案库”却非常庞大，能表达更多细节。然后，PQGAN把这种方法用到图像压缩和生成里，先把图片压缩成这种“拼图小块”的组合，再用扩散模型“画”出高质量的图片。结果是，PQGAN不仅让压缩后的图片更接近原图，还能让生成的图片更清晰、更真实，而且还能提高生成速度或生成更大尺寸的图片，既省力又高效。 
## Smart-GRPO: Smartly Sampling Noise for Efficient RL of Flow-Matching Models 
2025-10-03｜UCLA, Brown 

<u>http://arxiv.org/abs/2510.02654v1</u>  
### 概述 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/64.jpg)  
本文针对流匹配（flow-matching）生成模型在强化学习中的应用难题，提出了一种名为Smart-GRPO的高效噪声采样优化方法。流匹配模型以其训练稳定和确定性采样的优势，在文本生成图像领域表现优异，但其确定性特性限制了强化学习中策略优化所需的随机性。现有方法通过随机噪声扰动引入随机性，但效率低下且训练不稳定。Smart-GRPO创新性地将噪声采样视为优化变量，利用奖励信号指导噪声分布的迭代更新，优先采样高奖励噪声，从而加速强化学习收敛并提升生成图像质量。该方法无需修改模型架构，易于集成到现有的强化学习与人类反馈训练流程中，为流匹配模型的高效人类偏好对齐提供了实用路径。  

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/65.jpg)  
Smart-GRPO基于Flow-GRPO框架，核心在于优化噪声扰动的采样分布以提升策略优化效率。具体步骤包括：  

1. **初始化噪声分布**：以均值为零、单位方差的高斯分布作为初始噪声采样空间。  
2. **采样与扰动**：从当前噪声分布中采样多个候选噪声，利用这些噪声对潜变量进行扰动，形成一组候选生成图像的近似解。  
3. **奖励评估**：通过预训练的奖励模型对每个候选图像进行评分，衡量其质量与人类偏好对齐程度。  
4. **分布更新**：选取奖励最高的部分噪声样本，计算其均值和方差，用以更新噪声分布参数，实现向高奖励区域的迭代收敛。  
5. **迭代优化**：重复上述采样-评估-更新过程多轮，逐步精炼噪声分布，最终选取优化后的均值作为训练中使用的噪声扰动。  
此方法通过交叉熵法（CEM）式的迭代搜索，有效平衡探索与利用，避免了随机噪声带来的训练信号浪费，实现了强化学习中更稳定和高效的策略优化。  

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/66.jpg)  
实验部分对Smart-GRPO在文本到图像生成任务中的表现进行了系统评估。采用了ImageReward和Aesthetic Score两种奖励函数，分别衡量语义对齐和视觉美学效果。基线包括未微调的Stable Diffusion模型及使用Flow-GRPO微调的模型。结果显示，Smart-GRPO在奖励优化和生成质量上均优于基线，训练过程更稳定且收敛速度更快。消融实验验证了迭代细化和贪婪噪声选择对性能提升的关键作用，随机选择噪声导致训练不稳定甚至崩溃。此外，敏感性分析表明迭代次数的增加显著提升性能和训练稳定性，五次迭代达到较佳平衡。虽然方法依赖于奖励函数的准确性，且早期噪声层的奖励估计存在局限，但整体实验结果证明了Smart-GRPO在强化学习中优化流匹配模型噪声采样的有效性和实用性。  

### 通俗易懂  
Smart-GRPO的核心思想是“聪明地选择噪声”。在流匹配模型中，生成图像时需要给潜在变量加上一点随机“噪声”，这些噪声会影响最终图像的质量。以往的方法都是随机地选噪声，很多噪声其实没用，浪费了训练时间。Smart-GRPO通过一个“挑选噪声”的过程来改进这一点：它先随机选一批噪声，用这些噪声生成图像，然后用一个奖励模型给每张图像打分，挑出表现最好的噪声，再根据这些好的噪声调整噪声的分布。这样，下一轮采样时就更有可能选到好噪声。经过多轮这样的筛选和调整，噪声分布逐渐“聪明”起来，训练时用的噪声效率更高，模型学得更快，生成的图像也更符合人类的审美和需求。简单来说，Smart-GRPO就像是在一堆随机数中找宝石，不断调整搜索方向，最终找到最闪亮的那个，帮助模型生成更好看的图片。 
## Dale meets Langevin: A Multiplicative Denoising Diffusion Model 
2025-10-03｜IISc 

<u>http://arxiv.org/abs/2510.02730v1</u>  
### 概述 
  
本文探索了生物神经系统中遵循Dale定律的学习机制与生成模型之间的联系。Dale定律指出，兴奋性和抑制性突触在学习过程中不会互换角色，导致突触权重呈对数正态分布。然而，传统的梯度下降方法并不符合这一规律。作者基于指数梯度下降（EGD）与几何布朗运动（GBM）之间的数学联系，提出了一种基于GBM的乘法去噪扩散模型。该模型通过反向时间随机微分方程（SDE）的离散化，导出了与EGD更新规则结构相同的乘法采样更新方案。进一步，作者设计了适用于对数正态分布数据的乘法得分匹配损失函数，使得该生成模型能够有效学习图像数据的概率分布。该工作首次将生物启发的乘法更新机制引入生成模型领域，拓展了扩散模型的理论与应用边界。  

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/67.jpg)  
本文方法主要分为以下几个核心步骤：  

1. **生物启发的优化机制**：基于Dale定律，采用指数梯度下降（EGD）保证权重符号不变，权重分布趋向对数正态。EGD的乘法更新形式与GBM的采样过程紧密相关。  
2. **几何布朗运动建模**：定义前向SDE为带有乘法噪声的GBM，描述数据样本向对数正态分布的演变。通过对数变换，将GBM转化为线性SDE，便于推导反向时间SDE。  
3. **反向时间SDE离散化**：对反向SDE进行欧拉-马鲁雅玛离散化，得到乘法形式的样本更新规则，与EGD更新结构一致，实现由对数正态噪声到目标分布的采样。  
4. **乘法得分匹配损失**：提出乘法显式与去噪得分匹配损失函数，适配正值数据的对数正态分布，保证模型能有效估计反向SDE中所需的得分函数。  
5. **神经网络训练**：利用乘法去噪得分匹配损失训练神经网络逼近得分函数，支持基于乘法更新规则的样本生成。  

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/68.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/69.jpg)  
作者在MNIST、FashionMNIST和Kuzushiji三个图像数据集上验证了所提乘法去噪扩散模型的生成能力。实验结果显示，该模型能够生成清晰且多样化的样本，表现出良好的生成质量和稳定性。与传统基于加性噪声的扩散模型相比，乘法模型在保持数据非负性和符合生物学分布特性方面具有天然优势。模型训练过程中，乘法得分匹配损失有效引导神经网络学习对数正态数据的概率结构，提升了采样效率和生成效果。此外，实验还验证了该方法对Dale定律的遵循，权重分布符合对数正态，支持了理论推导的合理性。综上，实验充分证明了该生物启发乘法扩散模型在图像生成任务中的实用性和创新性。  

### 通俗易懂  
本文的方法灵感来源于大脑神经元的学习规律——神经元的连接强度（权重）不会随学习而改变“性质”，即兴奋的连接始终兴奋，抑制的连接始终抑制。传统机器学习中的梯度下降更新权重时，权重的正负号可能会改变，这不符合生物现象。为了解决这个问题，作者采用了一种“乘法式”的更新方法，每次调整权重时，是在原来的基础上乘以一个正数，从而保证权重符号不变。数学上，这种更新对应于一种叫做几何布朗运动的随机过程，其最终权重自然呈现对数正态分布。为了生成新数据，作者设计了一个特殊的“乘法得分匹配”训练方法，训练一个神经网络来估计数据分布的“方向”。这样，在生成新样本时，模型就能通过乘法更新一步步“逆转”噪声过程，从随机的对数正态噪声中恢复出真实数据。简单来说，这种方法模仿了大脑的学习规则，用一种特别的乘法方式来更新和生成数据，使得生成的样本既符合生物规律，也具备良好的质量。 
# Topic: Image Generation｜Safety
## Latent Diffusion Unlearning: Protecting Against Unauthorized Personalization Through Trajectory Shifted Perturbations 
2025-10-03｜UB, Adobe Research, UMBC 

<u>http://arxiv.org/abs/2510.03089v1</u>  
<u>https://github.com/naresh-ub/unlearnable_samples</u> 
### 概述 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/70.jpg)  
本文针对文本到图像扩散模型在个性化定制过程中存在的未经授权使用和隐私泄露问题，提出了一种新颖的防护机制。当前个性化技术能够利用少量用户图像快速高质量地定制模型，但也带来了版权和数据隐私的风险。已有的“不可学习”样本生成方法多在像素空间添加噪声，导致图像质量下降且易被净化攻击破解。本文创新性地在扩散模型的潜在空间中进行扰动，通过改变去噪轨迹的起点，生成视觉上与原图高度相似但对下游个性化任务无效的样本，有效防止未经授权的模型微调和复制。该方法兼顾了图像的不可察觉性与抗净化攻击能力，填补了现有技术在鲁棒性和视觉质量上的不足，保障了版权方和内容创作者的权益。

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/71.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/72.jpg)  
本文方法基于潜在扩散模型（Latent Diffusion Models, LDM），核心思想是对扩散过程中的潜在表示进行结构化扰动，具体步骤包括：  

1. **潜在反演**：将输入图像编码为潜在空间的噪声向量，即扩散过程的终止状态。  
2. **轨迹扰动**：利用参数化的单层U-Net网络对该终止噪声向量进行扰动，产生扰动后的潜在噪声。  
3. **少步去噪重构**：采用少步（如4步）去噪过程对扰动后的潜在噪声进行还原，生成最终的“不可学习”潜在表示。  
4. **个性化干扰目标**：通过最大化个性化模型（如Textual Inversion或DreamBooth）的损失，训练扰动网络，使得生成的样本在个性化微调中表现出低效能。  
5. **约束优化**：结合感知相似度指标（PSNR、SSIM、FID）限制扰动幅度，确保生成图像视觉上与原图高度一致，同时保证扰动对个性化任务的破坏性。  
该方法利用少步去噪的特性，避免了多步去噪的过度恢复，确保扰动信息在重构过程中得以保留，实现了扰动的隐蔽性和鲁棒性。

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/73.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/74.jpg)  
实验在四个公开数据集（CelebA-HQ、VGGFace2、WikiArt和DreamBooth）上进行，涵盖人脸和非人脸图像。结果显示，本文方法在图像质量指标（PSNR提升8dB以上，SSIM和FID显著优于对比方法）和个性化抵抗指标（身份匹配度显著下降，检测失败率大幅提升）上均取得领先。对比了多种现有基于像素空间扰动的方法，本文方法生成的样本在视觉上更自然且更难被DiffPure等强净化攻击恢复。跨不同版本的Stable Diffusion模型测试，方法表现出良好的泛化能力。消融实验揭示，扰动幅度和去噪步数的合理选择对性能至关重要，10/255的扰动预算和4步去噪为最佳组合。定量和定性分析均证明该方法在保护版权和隐私方面的实用价值和鲁棒性。

### 通俗易懂  
这项研究的核心是让图像“悄悄地”变得难以被AI模型用来学习和模仿，但人眼看不出差别。传统方法直接在图片上加噪声，结果图片会变得模糊或有杂点，且容易被专门的“清理”算法修复。本文的方法则是在AI模型内部的“思考空间”里动手脚。具体来说，AI生成图像的过程就像是在一条路径上逐步去掉噪声，恢复清晰图像。我们在这条路径的起点偷偷改变一些参数，确保最终生成的图像看起来几乎没变，但AI模型在用这些图片训练时，学不到原本的关键信息。这样，即使有人想用这些图片来定制自己的AI模型，也会失败。通过只用少量步骤去“清理”，我们既保证了图片的高质量，也让这种“干扰”难以被消除。简单来说，就是让图片变成“隐形的陷阱”，保护创作者的版权和隐私，同时不影响图片本身的美观。 
## Secure and Robust Watermarking for AI-generated Images: A Comprehensive Survey 
2025-09-30｜Queen’s University 

<u>http://arxiv.org/abs/2510.02384v1</u>  
### 概述 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/75.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/76.jpg)  
随着生成式人工智能（Gen-AI）技术的迅猛发展，尤其是在图像生成领域，AI生成的高质量图像广泛应用于艺术创作、内容生产等多个领域。然而，这也带来了知识产权保护、内容真实性验证以及责任追踪等严峻挑战。数字水印技术作为一种嵌入隐藏信息以验证图像来源和版权的手段，成为解决这些问题的关键工具。本文全面综述了AI生成图像水印技术的现状和发展，聚焦五大核心维度：（1）图像水印系统的形式化定义；（2）多样化水印技术的分类与比较；（3）视觉质量、容量和可检测性等评估方法；（4）针对恶意攻击的脆弱性分析；（5）当前面临的挑战与未来发展方向。通过系统性梳理，文章旨在为研究者提供对AI图像水印技术的全局理解，推动该领域的安全性和鲁棒性发展。

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/77.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/78.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/79.jpg)  
本文将AI生成图像水印系统形式化为七个核心模块：密钥生成（Setup）、编码（Code）、内容生成（Gen）、水印嵌入（Embed）、攻击通道（Channel）、水印提取（Extract）和验证（Verify）。  

1. **密钥与编码**：通过密码学密钥生成安全秘钥，用于派生水印载体和掩码，保证嵌入和验证的安全性。编码模块将消息转为带冗余的码字以提升鲁棒性。  
2. **内容生成与嵌入**：水印可在生成过程中（如潜变量空间或模型参数中）直接注入，也可在生成后对图像像素或频域系数进行后置嵌入。嵌入机制包括加法、调制、量化和掩码等多种方式，确保水印难以察觉且不影响图像质量。  
3. **攻击模型**：考虑常见图像处理（压缩、裁剪、噪声）及高级攻击（再生成攻击、对抗样本），评估水印的鲁棒性和安全性。  
4. **提取与验证**：基于密钥的提取器恢复水印码字，结合统计判决实现真假判别，保证误报率在可控范围内。  
5. **安全性定义**：强调水印的不可伪造性和隐蔽性，确保攻击者无法无授权复制、篡改或检测水印。  
该方法体系兼顾嵌入的隐蔽性、鲁棒性和安全性，适应生成式模型的特性，提供了一个统一且系统化的设计框架。

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-09/80.jpg)  
本文通过大量实验评估了不同水印技术在视觉质量、容量和检测准确性方面的表现。实验设计涵盖多种攻击场景，包括常见图像变换（如裁剪、压缩、旋转）和针对性攻击（如利用其他生成模型再生成图像以去除水印）。结果显示，生成内嵌式水印（如潜变量或模型参数扰动）在保持图像自然性的同时，表现出较强的鲁棒性和防伪能力。相比之下，传统后置像素或频域水印更易受常规图像处理影响。安全性实验则通过模拟攻击者尝试伪造或非法提取水印，验证了基于密钥的系统设计在防止伪造和非法读取方面的有效性。此外，实验还考察了水印容量与图像质量的权衡，确认合理设计可实现高容量且不影响视觉体验。整体实验为水印系统的实际部署提供了理论和实践依据。

### 通俗易懂  
水印技术就像给AI生成的图片“贴上隐形标签”，别人看不出来，但你自己用专门的“钥匙”能找到这个标签，确认图片是你生成的。这个过程分两步：第一步是“贴标签”，就是在图片生成时或生成后，把秘密信息悄悄藏进去；第二步是“找标签”，用特定方法检测这些秘密信息，判断图片是不是AI做的。为了让标签不容易被破坏，设计时会考虑各种可能的“破坏手段”，比如有人裁剪图片、压缩图片，甚至用别的AI重新生成图片，想把标签抹掉。这个系统还要保证只有你有钥匙能看到标签，别人不能随便复制或伪造标签。这样无论图片被怎么处理，你都能证明它是你生成的，也能防止别人冒用你的作品。整个方法就像给每张AI图片装上一个既隐秘又坚固的身份证。 
