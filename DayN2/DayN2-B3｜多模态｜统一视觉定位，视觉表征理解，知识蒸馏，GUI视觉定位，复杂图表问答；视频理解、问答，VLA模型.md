
# Topic: Multi-modal 

## UGround: Towards Unified Visual Grounding with Unrolled Transformers 
2025-10-04｜FDU, BEDICloud, ZJU, SJTU 

<u>http://arxiv.org/abs/2510.03853v1</u>  
<u>https://github.com/rui-qian/UGround</u> 
### 概述 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/191.jpg)
本文提出了UGround，一种统一的视觉定位框架，旨在解决当前视觉语言模型（LMMs）视觉定位任务中的两大瓶颈：一是传统方法固定使用最后一层隐藏状态作为文本提示，导致层层误差累积且缺乏中间校正；二是使用\<SEG\>标记隐式地将文本嵌入视觉空间，缺少明确的空间信息。UGround通过动态选择变换器中间层的相似性图作为“掩码提示”，为视觉模型（如SAM）提供显式空间线索，实现从传统表达式分割到推理分割、单目标到多目标、正向查询到空目标的任务统一。该方法基于策略引导掩码机制（Policy-Prompted Masking），包括随机跳层连接（SSC）和掩码提示（MasP），有效提升了视觉定位的准确性和泛化能力，首次从属性视角实现了视觉定位任务的统一。



### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/192.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/193.jpg)
UGround的核心是Policy-Prompted Masking（PPM），主要包括两个部分：

1. **随机跳层连接（SSC）**：将选择中间层作为文本\<SEG\>标记的表示视为强化学习任务，策略根据当前状态（各层\<SEG\>隐藏状态）分布采样连接层，实现动态跳过部分层，形成类似跳跃连接的结构。该策略利用REINFORCE算法优化，奖励基于所选层的相似性图与真实掩码的匹配度，增强模型对不同层信息的利用。
2. **掩码提示（MasP）**：给定选定层，计算\<SEG\>标记与图像token的相似度，得到二维相似性图，经过高斯平滑后作为软掩码提示输入SAM，提供明确的空间激活区域。该掩码不仅在前向传播中作为提示，还通过交叉熵损失约束反向传播，显式指导模型关注正确区域。
整体上，PPM机制通过动态层选择与显式空间提示，打破传统固定层提示的限制，提升了视觉定位的鲁棒性和准确率。



### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/194.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/195.jpg)
实验部分在多个视觉定位任务数据集上验证了UGround的有效性，包括ReasonSeg推理分割数据集、传统RefCOCO系列数据集及其多目标扩展gRefCOCO。结果显示，UGround在IoU和准确率指标上均显著优于当前最先进方法，最高提升达9.0%和12.1%。具体分析表明，动态选择中间层的策略显著提升了相似性图的判别力，加快了模型训练收敛速度。此外，利用相似性图作为软掩码提示，不仅增强了空间信息表达，还提高了模型对复杂查询（如多目标及空目标）的处理能力。消融实验进一步确认了随机跳层连接和掩码提示两部分对性能提升的贡献，展示了UGround在统一视觉定位任务中的广泛适用性和优越表现。



### 通俗易懂
UGround的核心想法是让模型不再只盯着“最后一层”的信息，而是灵活地从“中间层”挑选最有用的线索来帮助定位。想象一下，传统方法像是在传话游戏中，只听最后一个人说的话，信息可能失真；而UGround则允许直接听到中间某个人的话，避免信息丢失。具体做法是用一种智能策略，让模型自己决定从哪一层提取线索，这个过程像是在玩“跳房子”，跳过不重要的步骤，直接连接到最关键的部分。同时，它会计算文本和图像中每个小块的相似度，生成一张“热力图”，告诉视觉模型哪里可能是目标，就像给地图标注重点区域一样。这个热力图不仅告诉模型“看这里”，还在训练时让模型知道“看错了要改正”，这样模型学得更快更准。总的来说，UGround让视觉模型能更聪明地选信息、用信息，从而更准确地找到图像中对应文本描述的区域。 
## Visual Representations inside the Language Model 
2025-10-06｜Washington, UCLA, Allen Institute for AI 

<u>http://arxiv.org/abs/2510.04819v1</u>  
### 概述 
  
本文聚焦于多模态语言模型（MLMs）内部视觉表征的理解，特别是其视觉键值（key-value）缓存中的信息流动与表现。尽管现有研究多关注视觉编码器或变换器激活输出，MLMs在感知密集任务（如分割、语义对应、时序对应和指代表达检测）上的表现依然有限。作者通过分析三款主流MLMs（LLaVA-OneVision、Qwen2.5-VL和Llama-3-LLaVA-NeXT）中的视觉键值令牌，发现这些令牌本身已编码足够的视觉信息以支持多种零样本感知任务，且其性能与整体模型感知能力相关。然而，语言模型对输入视觉信息的增强有限，且在某些任务上不及未经多模态微调的视觉编码器（如SigLIP）。此外，后期层中的输入无关视觉键存在干扰信息，反而削弱了模型感知性能。研究还提出通过文本前缀引导视觉信息，改善视觉表征的感知能力，并指出若语言模型能更好地控制其视觉信息，整体感知性能将显著提升。该工作为MLMs的机制解释和视觉编码训练提供了新的视角和方向。

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/163.jpg)  
本文的方法主要围绕对MLMs内部视觉键值令牌的深入解析和操控展开，具体包括：  

1. **视觉信息流分析**：通过六个感知任务（前景分割、共分割、语义分割、指代表达分割、语义对应、时序对应）对各层视觉值令牌进行零样本性能评估，揭示视觉信息在语言模型层间的演变规律。  
2. **对比分析**：将语言模型中的视觉表征与视觉编码器（特别是未经多模态微调的SigLIP）输出进行性能对比，评估多模态微调对视觉信息的影响。  
3. **输入无关视觉键的识别与干扰研究**：通过统计不同图像样本中视觉键的方差，识别输入无关的视觉键，进而通过注意力干预阻断其影响，验证其对模型感知性能的负面作用。  
4. **视觉信息控制实验**：利用语言模型的因果跨模态注意力机制，向视觉输入添加相关文本前缀，观察视觉表征对感知任务性能的提升，验证视觉表征的可调性和适应性。  
5. **视觉信息利用率评估**：通过比较视觉值令牌对任务的正确预测与模型最终输出的正确率，揭示语言模型对视觉信息的利用不足，指出潜在的改进空间。

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/164.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/165.jpg)  
实验部分涵盖了多层次的视觉表征评估和干预验证。首先，通过在六个感知任务上对视觉值令牌的零样本测试，发现语言模型中的视觉信息在前期层逐渐丰富，后期层显著下降，且整体性能优于输入视觉编码器输出但不及未经多模态微调的SigLIP。接着，利用MSCOCO数据集识别输入无关的视觉键，发现其在语言模型早期和后期层反复出现。通过阻断后期层输入无关视觉键的注意力查询，模型在POPE和MME两个基准上的感知表现显著提升，证明这些键含有干扰信息。此外，文本前缀实验显示，添加与任务相关的文本提示能有效提升视觉值令牌在指代表达分割、语义对应及领域适应任务上的表现，而随机或错误前缀则无益甚至有害。最后，通过对比视觉值令牌与模型输出的正确率，发现语言模型大量未能充分利用已编码的视觉信息，尤其在艺术风格识别任务中，潜在提升达33.3%。

### 通俗易懂  
这项研究主要是想弄清楚多模态语言模型（MLMs）是如何“看懂”图片的。通常，模型先用视觉编码器把图片变成一串数字（视觉令牌），然后送到语言模型里“理解”。作者发现，语言模型内部保存了这些视觉令牌的“记忆”，这些视觉记忆其实已经包含了不少图片信息，足够做一些复杂的视觉任务，比如分割图片中的物体或找出图片间的相似点。  
不过，模型后面处理这些视觉信息时，有些“噪声”或无关的信息会混进来，反而影响了最终表现。研究人员通过“屏蔽”这些无关信息，模型表现反而变好了。更有趣的是，如果给图片加上相关的文字提示，模型能更好地利用视觉信息，任务表现也会提升。  
总结来说，模型内部的视觉信息很丰富，但模型没有完全用好它。通过更聪明地控制和引导这些视觉记忆，比如加上文字提示或过滤掉干扰信息，可以让模型更聪明地“看懂”图片，提升它在视觉相关任务上的表现。 
## Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs 
2025-10-05｜AAII, UTS(Sydney) 

<u>http://arxiv.org/abs/2510.04142v1</u>  
<u>https://anonymous.4open.science/r/Autonomous-Distillation/</u> 
### 概述 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/166.jpg)  
本研究聚焦于多模态大语言模型（MLLMs）知识蒸馏中的核心难题——多教师模型推理路径的概念漂移问题。多教师模型在推理过程中表现出动态且不可预测的分布变化，导致学生模型继承偏差，影响其性能和稳定性。针对这一挑战，论文首次将概念漂移理论引入多教师知识蒸馏，视推理过程为多流次令牌预测，揭示了多教师模型间的异步漂移及其对学生模型的影响。基于此，提出“学习-比较-批判”范式，通过自主偏好优化（APO）引导学生模型在多教师监督下自我蒸馏，筛选一致且优质的推理路径，同时批判性反思教师模型的漂移偏差，实现概念对齐。该方法显著提升了学生模型的鲁棒性、一致性和泛化能力，并贡献了大规模医学影像推理数据集CXR-MAX，推动多教师知识蒸馏研究。

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/167.jpg)  
方法部分基于多流概念漂移理论，系统处理多教师MLLMs推理路径的异步漂移，具体包括：  

1. **多流概念漂移建模**：将每个教师模型的推理过程视为独立的自回归序列，定义多流漂移以捕捉教师间动态分布差异，量化推理状态的概率分布演变。  
2. **监督预蒸馏**：学生模型在多教师提供的异构预测分布上进行预训练，吸收多元领域知识，形成初步的联合表示，缓解单一教师知识局限。  
3. **自我蒸馏概念对齐**：学生模型基于多教师生成的推理序列进行自我蒸馏，比较并提炼一致的推理路径，去除教师间的冲突信号，实现内部知识统一。  
4. **自主偏好优化（APO）**：借鉴强化学习和偏好优化，学生模型将自我蒸馏的推理视为正向偏好，教师漂移输出视为负向偏好，通过最大似然优化调整策略，批判性修正偏差，强化模型的稳定性和泛化能力。  
5. **CXR-MAX数据集构建**：为支持研究，构建涵盖7个公开MLLM教师推理轨迹的170,982实例医学影像数据集，促进领域特定多教师知识蒸馏的实证研究。

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/168.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/169.jpg)  
实验在医学胸片诊断领域展开，利用MIMIC-CXR数据集的1/10数据量进行训练，验证模型在鲁棒性、一致性和泛化能力上的表现。结果显示：  

1. **多教师蒸馏性能优越**：学生模型在多标签疾病分类任务中平均准确率达到0.78，超越所有单一教师模型及多种基线方法，尤其在教师间漂移较大的疾病类别表现显著提升。  
2. **推理一致性显著增强**：生成的诊断报告在BLEU、ROUGE-L和METEOR指标上均优于对比模型，表明模型能有效整合多教师知识，生成语义连贯且准确的推理文本。  
3. **强泛化能力**：在多项零样本分类任务中，学生模型表现持续优于主流基线，展现出良好的跨域适应能力。  
4. **消融实验验证模块有效性**：去除多教师模块或APO均导致性能下降，尤其APO显著提升了模型对漂移教师偏差的抵抗能力，强化了知识融合的稳定性。  
5. **数据集贡献**：CXR-MAX数据集为多教师知识蒸馏提供了丰富的实证基础，支持后续研究与应用。

### 通俗易懂  
本方法的核心是让学生模型学会从多个“老师”那里学知识，但这些老师有时会给出不同甚至相互矛盾的答案，就像几个医生对同一张X光片有不同看法。为了解决这个问题，方法分三步走：  
第一步，学生先听多个老师讲解，尽量吸收所有知识，虽然有些内容可能不完全一致；  
第二步，学生自己把这些不同的讲解放在一起比较，找出大家都认可的“共识”部分，把这些好的内容留住，舍弃不一致或错误的部分；  
第三步，学生再用一种“批判性思考”的方式，主动判断哪些老师的观点更靠谱，哪些是偏差，反复调整自己的理解，最终形成一个既全面又稳定的知识体系。  
这样，学生不仅学到了更多知识，还避免了盲目接受错误信息，变得更加聪明和可靠。这个过程就像学生在听不同老师授课后，自己总结出最合理的答案，同时还能识别和修正老师们的错误，保证学到的知识既准确又实用。 
# Topic: Multi-modal｜Image 
## GUI-Spotlight: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding 
2025-10-05｜UMN, Cisco Research, LLNL 

<u>http://arxiv.org/abs/2510.04039v1</u>  
<u>https://github.com/bin123apple/GUI_Spotlight</u> 
### 概述 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/170.jpg)  
本文针对多模态大语言模型在图形用户界面（GUI）视觉定位中的不足，提出了一种名为GUI-SPOTLIGHT的视觉聚焦模型。当前GUI代理在复杂、高分辨率界面上缺乏精细的视觉定位能力，难以实现精准的点击、拖拽等像素级操作。GUI-SPOTLIGHT通过动态调用多种专用视觉工具，采用迭代式聚焦策略，逐步缩小关注区域，显著提升了视觉定位的准确率。该方法在高分辨率的ScreenSpot-Pro基准测试中，以仅18.5K训练样本实现52.8%的准确率，超过了训练样本数百万级的现有7B模型，展现出优异的数据效率和泛化能力。研究同时对多工具协调的强化学习训练过程进行了改进，提升了训练稳定性和样本利用率。本文贡献在于提出了结合多工具迭代聚焦的视觉定位框架，设计了改进的多工具强化学习算法，并系统总结了训练过程中的经验和负面结果，为构建高效、稳健的GUI视觉定位模型提供了实用指导。  

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/171.jpg)  
GUI-SPOTLIGHT方法核心是“思考图像”，通过多轮迭代调用三种视觉工具（extract、find color、crop）来逐步缩小目标区域，提升定位精度。  

1. **推理流程**：模型接收文本描述和原始截图，维护一个图像注册表和对话历史。每轮模型输出工具调用或终止指令。工具执行后返回裁剪图像及偏移量，更新对话历史，直到模型输出最终坐标。  
2. **视觉工具设计**：  
   - *extract*：根据位置裁剪图像的四分之一区域，粗略缩小关注范围。  
   - *find color*：通过颜色匹配滑动窗口定位与目标颜色最接近的区域，实现颜色引导的聚焦。  
   - *crop*：根据指定顶点坐标裁剪矩形区域，实现精细聚焦。  
3. **训练策略**：分三阶段进行，第一阶段用监督学习预热模型，第二阶段采用改进的Group Sequence Policy Optimization强化学习优化多工具调用策略，第三阶段用高分辨率数据微调，提升模型对复杂界面的适应性。  
4. **奖励设计**：结合五种奖励，包括最终答案准确性、裁剪区域与目标的重叠度、颜色匹配成功率及工具调用格式正确性，综合引导模型稳定高效训练。  

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/172.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/173.jpg)  
实验在多个公开基准上验证了GUI-SPOTLIGHT的有效性。  
- **ScreenSpot-Pro**：在高分辨率专业软件界面视觉定位任务中，模型以仅18.5K训练样本达到52.8%准确率，超越了训练样本多达百万级的7B模型，展现出卓越的数据效率和跨领域泛化能力。  
- **UI-Vision**：针对多应用程序桌面界面，GUI-SPOTLIGHT相较原始UI-TARS-1.5-7B基线提升5.3个百分点，优于多种7B级开源模型，证明了模型在多样化复杂环境中的适用性。  
- **OSWorld-G**：在操作系统级任务的多样化截图中，模型表现出对文本匹配、元素识别和布局理解的综合提升，进一步验证了多工具强化学习策略的普适性。  
此外，消融实验显示多轮迭代推理显著优于单轮或重复单轮推理，强化学习算法和奖励设计的改进有效提升训练稳定性和最终性能。整体实验表明，GUI-SPOTLIGHT在视觉定位准确性、训练效率及泛化能力上均实现了显著突破。  

### 通俗易懂  
GUI-SPOTLIGHT的核心思想是像用手电筒一样“照亮”屏幕上的目标。想象你在一张复杂的地图上找一个小标记，直接找很难。这个模型会先用“提取”工具把地图分成四块，先看大致在哪一块；然后用“找颜色”工具根据颜色特征进一步锁定区域；最后用“裁剪”工具精确截取目标附近的小区域。每一步模型都会根据当前看到的图像和指令，决定下一步用哪个工具继续缩小搜索范围，直到找到准确的位置。  
训练时，模型先学习如何使用这些工具的基本操作（监督学习），然后通过强化学习不断试错，学会在不同情况下灵活选择工具和调整策略。为了让训练更稳定，设计了多种奖励机制，比如找到正确位置得分，调用工具格式正确也得分，鼓励模型既准确又规范。  
这样，模型就像一个聪明的探险家，逐步聚焦目标，既不浪费时间盲目搜索，也能在复杂界面中快速定位。相比直接一次性猜测，迭代聚焦让定位更精准、训练更高效，也更适合高分辨率和复杂的真实界面环境。 
## ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering 
2025-10-06｜J.P. Morgan AI Research 

<u>http://arxiv.org/abs/2510.04514v1</u>  
### 概述 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/174.jpg)  
ChartAgent针对复杂图表问答（ChartVQA）中的视觉推理难题提出了一种创新的多模态智能体框架。传统多模态大语言模型（MLLM）在处理未标注图表时性能急剧下降，主要因缺乏对图表空间信息的精确视觉理解。ChartAgent借鉴人类解读图表的认知策略，将自然语言查询拆分为多个视觉子任务，主动操作图表图像，执行如绘制标注、裁剪区域、定位轴线等专门动作，利用专门设计的图表视觉工具库辅助推理。该方法通过多轮交互迭代，模拟人类逐步理解图表的过程，实现视觉信息的精准捕捉和数值估算，显著提升了对未标注图表和数值密集查询的处理能力。实验结果表明，ChartAgent在ChartBench和ChartX两个权威基准上，较现有方法提升了16.07%整体准确率和17.31%未标注图表的数值问答准确率，且表现稳定，适用多种图表类型和复杂视觉推理场景，展现出良好的泛化能力和模块化扩展潜力。

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/175.jpg)  
ChartAgent构建于一个多轮交互的agentic框架，核心包含三个关键阶段：  

1. **思考（Thought）**：基于当前多模态状态（包括图表图像、用户问题及先前推理结果），MLLM生成下一个视觉子任务目标，如图例检测、饼图分割或轴线定位，部分任务涉及数值计算。  
2. **行动（Action）**：根据子任务目标，从模块化的图表视觉工具库中选择合适工具执行操作。工具库包含40余种通用与图表类型专属工具，如分割、检测、局部裁剪、数值插值等，工具输出结构化数据和可视化结果，便于后续验证。  
3. **观察（Observation）**：对工具输出进行视觉自检，评估分割完整性、标注匹配度及数值合理性。若结果不理想，系统会调整工具参数或切换工具，进行迭代优化，直到答案收敛或达到最大迭代次数。  
此外，系统通过图表元数据提取和少样本示例检索实现针对不同图表类型的定制化推理流程，支持对带注释和未注释图表的智能路由。该方法结合视觉推理与语言理解，形成闭环反馈机制，提升视觉信息的准确捕获和推理的可靠性。

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/176.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/177.jpg)  
实验在ChartBench和ChartX两个大型公开数据集上进行，涵盖40多种图表类型及丰富的数值与关系问答任务。数据集中高比例未标注图表和数值密集型问题，极大考验模型的视觉推理能力。ChartAgent与42个包括专有和开源的多模态大语言模型及图表专用模型对比，采用准确率和数值误差为主要评测指标。结果显示，ChartAgent在整体准确率上领先所有竞争模型，未标注图表问答准确率提升显著，尤其在复杂视觉推理和数值估算任务中表现卓越。进一步分析揭示，视觉自检机制成功提升了70%的推理恢复率，显著降低了错误传播。多轮迭代增强了模型对图表细节的理解，适应不同图表结构和问答复杂度。实验还验证了ChartAgent对底层MLLM的良好兼容性，支持模块化升级，展现出优异的泛化和扩展性。

### 通俗易懂  
ChartAgent就像一个聪明的图表助手，当你问它关于图表的问题时，它不会仅仅用文字猜答案，而是像人一样“看”图表，分步骤去理解。首先，它会把大问题拆成小任务，比如先找图例，再分辨饼图的每一块，或者测量柱状图的高度。接着，它会用一套专门的“工具箱”——里面有各种看图和测量的工具，帮助它完成这些小任务。完成后，它会检查工具的结果是不是靠谱，比如看分割的饼块是不是完整，颜色是不是对，数值有没有错误。如果发现问题，它会调整工具参数或换个工具，再试一次。这个过程就像人类读图时不断确认细节，直到确信答案准确为止。这样，ChartAgent能更准确地回答那些没有文字说明、复杂又难懂的图表问题，表现比传统模型好很多。它还能根据不同类型的图表，灵活选择合适的工具和策略，确保每次都能找到最合适的答案。 
# Topic: Multi-modal｜Video

## Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models 
2025-10-06｜Rochester, NWU(USA), CMU, UCSB, Purdue , NYU, Oxford, Brown, Virginia, Sony 

<u>http://arxiv.org/abs/2510.05034v3</u>  
<u>https://github.com/yunlong10/Awesome-Video-LMM-Post-Training</u> 
### 概述 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/69.jpg)  
视频理解作为计算机视觉中的前沿难题，要求模型具备处理复杂时空关系、长时依赖和多模态信息推理的能力。近年来，视频大型多模态模型（Video-LMMs）通过结合视觉编码器与强大的解码语言模型，实现了视频理解任务的显著进步。然而，将这些模型从基础感知系统转变为复杂推理引擎的关键阶段——后训练（post-training）在文献中尚无系统总结。本文综述了Video-LMM后训练的三大核心支柱：一是通过链式思考（Chain-of-Thought, CoT）进行的监督微调（SFT），二是基于可验证目标的强化学习（RL），三是利用增强推理计算的测试时扩展（TTS）。文章系统梳理了这些技术的功能定位、相互关系及其针对视频特有挑战（如时序定位、时空标注、长视频效率、多模态证据整合）的适配策略，归纳了设计原则、评估协议，并指出了奖励设计、可扩展性及成本性能优化等关键难题，同时整理了评测基准和数据集，为研究者提供统一框架以推动Video-LMM推理能力的提升。

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/70.jpg)  
Video-LMM后训练方法主要包括以下三部分：  

1. **监督微调（SFT）**：通过大规模标注数据，特别是链式思考格式的推理示例，增强模型的多模态对齐和指令执行能力。SFT不仅实现视觉与语言的有效融合，还为后续强化学习提供稳定的初始策略。其流程涵盖模态整合（将视觉特征映射至语言模型嵌入空间）、领域适应（从图像到视频，或特定领域视频的微调）、以及视频指令调优，逐步提升模型对视频内容的理解和响应能力。  
2. **强化学习（RL）**：采用不依赖人工偏好数据的Group Relative Policy Optimization（GRPO）等方法，通过可验证的奖励函数（如答案正确率、时序一致性）优化策略。RL方法强调多维度奖励设计、先进策略算法和高质量数据集的协同作用，提升模型在复杂视频推理中的泛化与自我纠错能力。视频特有的时序GRPO、回归GRPO及难度感知策略等变体，针对视频推理的时空特性和任务难度进行优化。  
3. **测试时扩展（TTS）**：通过增加推理计算资源，实现推理样本增强、多路径搜索、自洽性检验等技术，提升推理的准确性和鲁棒性。TTS方法使模型在推理阶段具备多轮反思和自主选择关键帧的能力，进一步强化视频内容的深入理解。

### 实验 
  
实验部分涵盖了多种Video-LMM后训练策略在不同数据集和基准上的评估。通过CoT格式的监督微调，模型在视频问答、时序定位和时空标注等任务中表现出显著提升，验证了结构化推理示例对模型能力的促进作用。强化学习阶段，采用GRPO及其变体的模型在多任务、多模态视频理解中实现了更高的准确率和更强的自适应能力，尤其在时序一致性和复杂推理任务中表现优越。测试时扩展技术则在推理稳定性和多样性上带来了明显增益，支持模型通过多路径投票和自我验证减少错误。整体实验展示了三阶段后训练体系的协同效果，验证了其在提升Video-LMM推理能力、处理长视频和复杂时空关系方面的有效性，同时揭示了奖励设计复杂性、训练成本和数据构建等实际挑战。

### 通俗易懂  
这篇论文主要讲的是如何让大型视频理解模型变得更聪明、更会推理。方法分三步：第一步是“监督微调”，就像给模型看很多有步骤的答案示范，教它怎么一步步思考视频内容。第二步是“强化学习”，模型自己尝试回答问题，并根据答案对错得到奖励，慢慢学会改进，不用人来告诉它哪个答案更好，而是用能自动验证的规则来判断。第三步是“测试时扩展”，就是在模型回答问题时多做几次尝试，像多个人投票一样，选出最靠谱的答案。这样，模型不仅能看懂视频，还能理解视频里发生的事情的时间顺序和空间关系，回答得更准确。这套方法让模型像人一样，既能看清楚画面，又能深入思考，解决复杂的视频理解问题。 
## A.I.R.: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering 
2025-10-06｜UCF, WCM 

<u>http://arxiv.org/abs/2510.04428v1</u>  
<u>https://github.com/UCF-AIR/A.I.R</u> 
### 概述 
  
本文针对视频问答（VideoQA）中关键的帧选择问题提出了A.I.R.方法。视频内容丰富且帧数庞大，直接处理所有帧计算量极大，不现实。现有方法多依赖轻量级模型（如CLIP）计算查询与帧的相似度，但这类模型难以捕捉复杂查询中的时序和语义细节，导致选择的帧与查询相关性不足。另一类方法利用大型视觉语言模型（VLM）深度分析帧与查询的关系，准确率高但计算成本极高。A.I.R.通过结合轻量模型的快速初筛和大型VLM的深度推理，采用自适应、迭代的框架，显著提升了帧选择的准确性和效率，兼顾复杂查询的理解与计算资源的合理利用，推动了视频问答技术的实用化进程。

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/178.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/179.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/180.jpg)  
A.I.R.方法分为三大阶段：  

1. **自适应初始采样**：基于轻量级模型CLIP计算视频中每帧与查询的相似度，采用高斯混合模型自适应阈值识别高相关的时间区间（事件），并根据事件长度动态分配采样帧数，保证覆盖关键时刻且避免冗余。  
2. **迭代帧选择**：针对初始采样得到的候选帧集，迭代执行四步循环：  
  - （1）区间潜力排序：将帧划分为时间区间，综合考虑区间内相似度、复杂度和长度评估潜力，选出高潜力帧。  
  - （2）基于VLM的推理分析：用大型VLM对候选帧进行细致语义推理，产生相关性评分和文本解释，筛除无关帧。  
  - （3）早停机制：当达到预设帧预算时停止迭代，节约计算资源。  
  - （4）局部密度采样：围绕已验证帧，从原始高帧率视频中以指数增长的采样步长采集更多细粒度帧，补充遗漏信息。  
3. **问答推理阶段**：最终选定的帧输入回答VLM进行问题回答。此方法自适应视频长度和复杂度，兼顾准确率与计算效率。

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/181.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/182.jpg)  
A.I.R.在多种长视频和短视频问答基准上进行了广泛评测，涵盖Video-MME、MLVU、LongVideoBench、EgoSchema和NextQA等数据集。实验结果表明，A.I.R.作为无训练、模型无关的插件模块，能显著提升多种基础VLM的性能，如QwenVL、InternVL-3、LLaVA-OneVision和VILA-1.5。与传统均匀采样和现有帧选择方法相比，A.I.R.在准确率上普遍领先，且所需分析帧数更少，计算效率明显提高。消融研究验证了各组件（自适应阈值、区间排序、推理分析、局部采样）的有效性。与其他基于VLM的帧选择方法相比，A.I.R.通过迭代机制有效降低了推理成本，实现了准确性和效率的最佳平衡。

### 通俗易懂  
A.I.R.就像是一个聪明的“视频导游”，帮你从一部很长的视频里挑出最关键的几张图片来回答问题。首先，它用一个快速但不太聪明的助手（CLIP）大致浏览视频，找到可能跟问题有关的时间段。然后，它会给这些时间段分配不同数量的“重点观察点”，保证覆盖重要内容。接下来，A.I.R.进入一个循环：每次它会挑出最有可能有用的画面，用一个超级聪明的大脑（大型VLM）仔细分析这些画面和问题的关系，筛掉无关的。它还会围绕这些关键画面，细致地寻找附近还没被注意到的细节。这个过程不断重复，直到找到足够多的关键画面为止。最后，把这些精选的画面交给大脑来回答问题。这样做既避免了盲目看所有画面浪费时间，也避免了只看表面相似画面而错过关键内容，既聪明又高效。 
## Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning 
2025-10-05｜UW-Madison, Microsoft, Columbia, USC, FDU 

<u>http://arxiv.org/abs/2510.04022v3</u>  
### 概述 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/183.jpg)  
本文提出了一种面向长视频问答（Long-Video QA）的新框架——Video-in-the-Loop（ViTL），旨在高效利用有限的计算资源，通过两阶段的“先粗后细”策略实现问题相关时段的精准定位与高质量答案生成。ViTL首先以低帧率对整段视频进行快速浏览，定位出与问题密切相关的时间片段；随后以更高帧率和分辨率对这些关键片段进行细致分析，生成最终答案。为解决长视频QA中缺乏明确时段标注的问题，作者设计了VGrounding-QA数据集，将事件知识图谱转换为带有时段监督的多项选择问答，确保训练过程中模型能够学习“在哪里看”与“如何答”相结合的能力。训练过程中，ViTL采用一种创新的联合优化策略，将定位的时段质量（IoU）与答案正确率紧密耦合，实现从答案反馈到时段定位的直接信用传递，显著提升了长视频QA和时段定位的性能。该方法在多个公开长视频QA及时段定位基准上均取得了领先成绩，展示了其在复杂长视频理解任务中的优势和可扩展性。

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/184.jpg)  
ViTL框架包括以下关键组成部分：  

1. **两阶段视频处理**：  
   - **阶段1（定位）**：以固定且较低帧率均匀采样整个视频，结合问题和由问题提炼的定位查询，预测一个或多个相关时间段（时段可以不连续），并生成简短的定位理由。  
   - **阶段2（回答）**：对阶段1定位出的时间段进行裁剪，并以更高的帧率和更高的视觉质量重新编码，仅利用这些关键帧推理生成最终的多项选择答案及其理由。  
2. **时序标注与文本时间戳注入**：每帧图像令牌后附带可读的绝对时间戳，稳定时间参照，辅助模型准确定位和回答。  
3. **联合训练策略**：采用基于Group-Relative Policy Optimization（GRPO）的强化学习框架，设计复合奖励函数，结合时段定位的IoU指标和答案正确率，实现答案对定位的反向指导。训练流程包括：  
   - 监督预热，分阶段微调定位和回答模块；  
   - 逐步连接两阶段预测，混合使用模型和真实时段作为输入；  
   - R1风格的强化学习后训练，强化答案正确性驱动的时段优化。  
4. **事件知识图谱辅助构建训练集**：通过语义分块和事件关系构建事件知识图谱，自动生成与视频时段对应的多项选择QA训练样本，保障训练中时段与答案的高度关联。  

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/185.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/186.jpg)  
作者在多个长视频问答和时段定位基准上验证了ViTL的有效性，包括LongVideoBench、LVBench、MLVU（长视频QA）以及Charades-STA和ActivityNet-Captions（时段定位）。实验结果显示：  
- ViTL在保持固定的计算预算和帧数输入下，相较于统一采样和单阶段方法，准确率提升显著（长视频QA提升最高达8.6%），同时帧数消耗减少约50%。  
- 在时段定位任务中，ViTL在Recall和mIoU指标上均超越了当前最先进的多模态大模型，显示出更强的时序定位能力。  
- 消融实验揭示，文本时间戳注入显著提升了模型对时间信息的理解能力，联合训练策略使得时段定位和答案生成协同优化，进一步提升整体性能。  
- 定性分析展示了ViTL能够生成连贯的推理过程和准确的时段定位，提供了可解释的问答结果。  
- 通过事件知识图谱构建的训练集保证了训练监督的准确性和多样性，促进模型在复杂多跳时序推理上的表现。  

### 通俗易懂  
ViTL的核心思想可以比喻成“先浏览，再聚焦”。当你面对一部很长的视频和一个问题时，你不会一帧一帧地看完整部视频，而是先快速扫视一遍，找到可能包含答案的关键片段。ViTL就是这样做的：第一步，它用较低的帧率快速浏览视频，找到几个可能相关的时间段；第二步，它只对这些选中的片段用更高的帧率和更清晰的画面仔细分析，最后给出答案。  
为了让模型知道“在哪里看”，研究人员还设计了一个特殊的数据集，每个问题都明确标注了答案所在的时间段，就像告诉模型“答案在第几分钟到第几分钟之间”。训练时，模型不仅学习如何回答问题，还学习如何准确地找到这些时间段。更厉害的是，模型的训练方式能让它明白，找到正确的时间段有助于答对问题，因此它会不断调整定位策略，确保“看对地方”。  
简单来说，ViTL就像一个聪明的观众，先快速寻找线索，再专注细看关键部分，最后给出答案，这样既节省了时间，也保证了答案的准确性。 
# Topic: Multi-modal｜Embodied 
## Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert 
2025-10-04｜ZJU, Shanghai AI Lab 

<u>http://arxiv.org/abs/2510.03896v1</u>  
### 概述 
  
本文针对视觉-语言模型（VLM）在物理世界中执行任务时的局限，提出了一种创新框架，通过引入“通用动作专家”实现高层规划与低层动作执行的有效解耦。传统的视觉-语言-动作（VLA）模型因训练数据稀缺且领域狭窄，导致泛化能力差，且将推理与动作整合在单一架构中，难以适应新环境。现有双系统方法虽尝试分离“思考”与“行动”，但动作模块面临语义歧义，限制了跨任务大规模训练。本文创新性地使用稀疏3D轨迹作为VLM与动作专家之间的中介表示，降低动作模块的语义负担，实现了动作专家对多任务和新环境的零样本泛化，显著提升了整体系统的适应性和执行效率。

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/187.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/188.jpg)  
本文方法基于三大核心设计：  

1. **稀疏3D轨迹接口**：VLM负责生成稀疏的3D关键路径点（waypoints）和最终执行器姿态，轨迹在相机坐标系下生成，避免了复杂的坐标变换，充分利用VLM的视觉先验和空间推理能力。  
2. **动作专家模块**：动作专家以轨迹点为指导，结合实时环境点云数据，利用条件扩散模型对稀疏轨迹进行细化，生成连续且可执行的动作序列。该模块专注于运动细节的精细调整，摆脱了高层语义解释的负担。  
3. **“动作预训练+点云微调”策略**：首先在大规模纯轨迹数据上进行预训练，掌握基础运动技能；随后利用点云数据进行微调，学习环境感知下的轨迹优化。这种分阶段训练显著提升了训练效率和泛化能力，支持动作专家在多样化环境中零样本部署。

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/189.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/190.jpg)  
在模拟环境（RoboTwin、ManiSkill）和真实机器人平台上，本文方法展示了卓越的性能和泛化能力。与多种主流模型和专家模型对比，本文系统在短、中、长任务时域均表现优异，尤其在长时域任务中成功率达60%，远超其他模型。泛化测试涵盖不同摄像机视角、未见颜色和新语义指令，均展现出强大的零样本适应能力。真实机器人实验中，动作专家保持冻结，仅对VLM进行少量监督微调，实现了多任务高效执行，优于传统基于逆运动学的执行方法。消融研究进一步验证了训练步骤、噪声水平、训练策略及点云数据源对模型性能的关键影响，强调了分阶段训练和高质量点云注释的重要性。

### 通俗易懂  
这项研究就像把机器人“头脑”和“手脚”分开训练。机器人用视觉语言模型（“头脑”）来规划大致的动作路线，比如告诉“手”去哪里抓东西，但不需要告诉“手”具体怎么动。然后，动作专家（“手”）根据这个大致路线和实时看到的环境，调整动作细节，让机器人动作更精准。训练时，先让“手”学会跟着路线走，再让它学会根据环境调整动作，这样“手”就能适应各种新环境，不用每次都重新教。这样分工让机器人既聪明又灵活，能在没有特别训练的情况下完成很多复杂的任务。 
