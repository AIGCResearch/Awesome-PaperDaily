
# Topic: Image Generation 

## Factuality Matters: When Image Generation and Editing Meet Structured Visuals 
2025-10-06｜CUHK, BUAA, Krea AI, SJTU, Shanghai AI Lab, HuggingFace, NUS, ByteDance, HKU 

<u>http://arxiv.org/abs/2510.05091v1</u>  
<u>https://structvisuals.github.io</u> 
### 概述 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/50.jpg)  
当前视觉生成模型在自然图像生成上表现优异，但在生成和编辑结构化视觉内容（如图表、数学图形、科学示意图）方面仍显不足。结构化视觉不仅要求图像美观，更需精确的构图规划、文本渲染和多模态推理以确保事实准确性。本文首次系统性地研究结构化图像的生成与编辑，涵盖数据构建、模型训练及评测基准。研究团队构建了一个包含130万高质量结构化图像对的数据集，数据来源于可执行绘图代码，并辅以链式思维推理注释。基于此数据，训练了一个结合视觉语言模型（VLM）和FLUX.1 Kontext的统一模型，通过轻量级连接器增强多模态理解能力。为评估模型性能，设计了包含1700多个挑战样本的StructBench基准及专门的StructScore评测指标，采用多轮问答方式细致测评生成图像的事实准确度。实验结果显示，尽管闭源系统表现领先，但整体性能仍有较大提升空间，且推理增强的模型在编辑任务中表现突出。通过公开数据集、模型和基准，推动结构化视觉领域的多模态基础模型发展。

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/51.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/52.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/53.jpg)  
本研究方法主要包含三个核心部分：  

1. **数据构建**：收集约200万条结构化图像绘制程序（Python、LaTeX等），执行代码生成源图像。利用GPT-5分析源图像，提取视觉显著特征，生成对应的图像编辑和代码编辑指令，执行代码编辑获得目标图像，实现严格的代码与图像对齐。数据经过多重过滤，确保高质量和丰富注释，最终形成130万对结构化图像数据。每个样本配备链式思维推理注释，提供详细的生成和编辑推理路径。  
2. **模型设计**：基于FLUX.1 Kontext扩展，采用轻量级MLP连接器将Qwen-VL编码的多模态特征与FLUX.1 Kontext骨干网络对齐，提升对结构化视觉输入的理解。文本指令编码采用T5，图像通过变分自编码器（VAE）编码，统一序列输入到扩散变换器中处理。  
3. **训练策略**：采用三阶段渐进式训练。第一阶段冻结骨干，仅训练连接器，实现特征对齐；第二阶段引入结构化图像数据联合微调骨干和连接器，采用基于掩码的损失加权策略，减轻背景和未变区域的影响，提升对结构化视觉的适应能力；第三阶段利用链式思维注释作为长上下文输入，注入显式推理能力，增强模型复杂任务的理解与生成能力。推理时进一步结合外部推理器，分步分析输入并指导生成，提升生成准确性。

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/54.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/55.jpg)  
实验评测涵盖15个主流开源与闭源模型，分别在结构化图像生成（StructT2IBench）和编辑（StructEditBench）两个基准上测试。结果显示，闭源模型整体表现优于开源模型，但即便是最先进的系统准确率也仅约50%-55%，显示结构化视觉任务的挑战性。训练数据的规模和质量对性能影响显著，本文模型在编辑任务上取得最高准确率，验证了数据驱动的重要性。不同视觉编码器和模型架构对性能影响不一，无明显优胜者。细分任务中，简单编辑（如颜色调整）准确率较高，复杂编辑（如图表类型转换）准确率显著下降，体现推理能力的关键作用。引入推理增强机制后，所有模型性能均有明显提升，尤其是在复杂编辑任务中，验证了推理推断在结构化视觉生成中的价值。评测指标StructScore通过多轮问答细粒度评估图像事实准确性，显著优于传统视觉相似度指标。实验结果表明，结构化视觉生成与编辑亟需结合推理能力和高质量数据支持。

### 通俗易懂  
这项研究解决了生成和修改结构化图像（比如图表、数学图形）时的难题。与普通照片不同，这些图像不仅要好看，还要准确表达数据和关系。研究团队先收集了大量能画出这些图像的程序代码，然后用智能模型分析图像内容，提取关键特征，接着生成对应的修改指令，最后自动修改代码生成新的图像。这样做保证了图像和代码一一对应，方便模型学习。模型设计上，他们用两个强大的视觉语言模型，通过一个轻巧的“桥梁”把两者连接起来，让模型能更好理解图像和文字的关系。训练时，先让模型学会把两种信息对齐，再让它学习结构化图像的知识，最后加入“思考”步骤，让模型能一步步推理图像内容和修改要求，提升准确度。推理时，还会用外部智能帮忙分析和规划，帮助模型更好地完成任务。实验结果显示，这种结合代码驱动数据和推理增强的训练方法，能让模型在处理结构化图像时表现更好，尤其是在复杂编辑任务中效果显著提升。简单来说，就是用程序代码做教材，教模型“认真思考”后再动手画图，这样生成的图既漂亮又准确。 
## OpusAnimation: Code-Based Dynamic Chart Generation 
2025-10-02｜OpusAI, Brown, ZJU, Toronto 

<u>http://arxiv.org/abs/2510.03341v1</u>  
### 概述 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/109.jpg)  
本文针对动态图表生成（Dynamic Chart Generation, DCG）任务，提出了首个专门评测多模态大语言模型（MLLM）在该领域能力的基准——DCG-Bench。动态图表相比静态图表，具有更高的信息密度和更丰富的动画叙事效果，广泛应用于视频制作和交互网页等场景，但现有研究多聚焦于静态图表生成，动态图表代码生成尚未充分探索。DCG-Bench涵盖三种任务类型：详细文本到图表（D2C）、简单文本到图表（S2C）和视频到图表（V2C），通过8K高质量样本构建数据集DCG-8K，包含指令、代码、视频及对应的问答对，支持从代码和视频两方面评价生成质量。实验揭示现有MLLM在视觉输入驱动的动态图表生成上表现不足，本文提出的训练方法显著提升模型性能，且以3B参数模型实现与大型专有模型相当的效果，推动动态图表生成技术发展。

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/110.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/111.jpg)  
本文设计了一个两阶段训练流程以构建高效的动态图表生成MLLM，核心方法包括：  

1. **监督微调（SFT）**：利用DCG-8K中详细文本到图表（D2C）和视频到图表（V2C）任务数据，对基础模型Qwen2.5-VL-3B进行微调，提升模型对复杂动态图表代码的生成能力。简单文本任务（S2C）未参与训练，用于测试模型泛化。  
2. **联合代码-视觉奖励（Joint-Code-Visual Reward, JCVR）**：设计多模态奖励函数，综合评估生成代码的功能正确性和渲染视频的视觉质量。奖励由代码质量分和视频质量分按一定比例加权组成，未能渲染视频的代码将被惩罚。  
3. **群体相对策略优化（Group Relative Policy Optimization, GRPO）**：基于JCVR奖励，采用GRPO强化学习策略优化模型生成策略，克服监督微调易过拟合长代码序列的问题，提高模型泛化能力和生成质量。  
该训练方案通过结合文本和视觉信号的多模态强化学习，增强模型对动态动画细节的理解和表达能力，实现跨任务和跨模态的知识迁移。

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/112.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/113.jpg)  
在DCG-Bench的三大任务上，本文方法构建的Qwen2.5-DCG-3B模型表现出显著优势。与主流专有模型如GPT-4.1、Claude和Gemini-2.5-Pro相比，Qwen2.5-DCG-3B在执行率和问答评分上均有竞争力，尤其在视频到图表任务中表现突出，超越了多数开源和部分专有模型。实验还验证了两阶段训练的有效性：SFT阶段为模型提供基础能力，JCVR-GRPO阶段进一步提升性能和泛化，避免了单纯监督微调的过拟合问题。消融研究表明，训练数据中同时包含详细文本和视频任务样本能显著提升模型的多任务表现；奖励函数中代码与视频质量的合理权重分配对性能提升至关重要。用户研究和错误分析进一步支持了模型生成代码的准确性和视觉效果的优越性，显示出该训练方法在动态图表生成领域的实用潜力和推广价值。

### 通俗易懂  
简单来说，本文想让计算机更聪明地画出会动的图表，比如那些能展示数据随时间变化的动画图。为了做到这点，研究人员先给计算机看了大量“图表动画”的例子，包括详细的文字说明、对应的代码和动画视频，让它学会理解复杂的说明并写出能生成动画的代码。接着，他们设计了一种特别的“奖励机制”，不仅看代码写得对不对，还要看动画效果好不好。计算机会生成多个版本的代码，系统会给每个版本打分，分高的代码会被奖励，分低的会被惩罚。通过不断尝试和调整，计算机逐渐学会写出既正确又漂亮的动画图表代码。这个方法比传统的只靠模仿示范的方法更聪明，因为它能从结果的好坏中学到更多，也能更好地应对不同类型的输入，比如文字或视频。最终，这种训练方法让一个参数较小的模型表现得和大模型一样好，既省资源又能完成复杂任务。 

## Zoom-In to Sort AI-Generated Images Out 
2025-10-05｜SJTU, Ant Group 

<u>http://arxiv.org/abs/2510.04225v1</u>  
### 概述 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/105.jpg)  
随着AI图像生成技术的迅猛发展，真实与合成图像的界限日益模糊，数字内容的真实性验证变得尤为重要。现有基于视觉语言模型（VLMs）的检测方法虽具解释性，但往往难以捕捉高质量合成图像中的细微伪迹，且多采用单次全局分析，导致关键局部线索被忽略。本文提出ZoomIn，一种模仿人类视觉检查的两阶段图像取证框架：首先对整图进行扫描，定位疑似合成区域；然后聚焦放大这些区域，进行细致分析，输出基于视觉证据的判定和解释。为训练此模型，构建了包含2万张真实与高质量合成图像的MagniFake数据集，配备了边界框和细粒度取证解释。实验表明，ZoomIn在准确率（96.39%）和泛化能力上显著优于现有方法，且能提供直观、基于图像细节的解释，推动了AI生成图像检测向更具解释性和精细化的方向发展。

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/106.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/107.jpg)  
ZoomIn框架核心在于两阶段推理流程，模拟人类专家的视觉放大检查：  

1. **全局扫描（Query1）**：输入整幅图像，VLM输出初步判定（真实/合成）、疑似区域的边界框及初步解释，重点识别对生成模型难以复现的区域（如细节丰富的人脸、手部、文字等）。  
2. **局部证据复核（Query2）**：基于Query1的边界框，提取图像局部裁剪图，与原图一同输入VLM，进行细节对比分析，生成最终判定与精细解释。此阶段强化了模型对局部伪迹的感知能力，纠正全局扫描可能的误判。  
为支持训练，设计了自动化数据注释流程：利用GPT-4o生成细粒度取证解释，Qwen-2.5-VL模型从解释中提取空间定位信息，形成带有图文结合的(I,E,B,C)训练元组。训练采用监督微调与基于BLEU指标的强化学习相结合的策略，提升模型在判定准确性和解释质量上的表现。该方法有效结合了视觉与语言的推理能力，强化了空间定位和多步验证机制。

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/108.jpg)  
在MagniFake测试集上，ZoomIn的7B和32B模型分别实现了94.2%和97.2%的检测准确率，显著优于包括LEGION和FakeShield等多种基线方法。通过两阶段的放大机制，模型在约10%的样本中纠正了初步判定，显示出较强的细节复核能力。解释质量指标BLEU和ROUGE-L均随放大机制提升，说明模型生成的解释更贴近人工注释。跨域测试（GenImage、MMFR、SynthScars）进一步验证了ZoomIn的泛化能力，32B模型平均准确率达90.8%。消融实验表明，去除放大机制或随机裁剪显著降低性能，强调了智能定位可疑区域的重要性。限制边界框数量过多也会影响效果，表明合理控制关注区域数目是关键。定性分析展示了模型在光照异常、解剖结构异常、纹理伪迹等典型合成特征上的敏感性，同时揭示了在极高真实感图像上的挑战。

### 通俗易懂  
ZoomIn方法就像一个细心的侦探在检查一张照片。第一步，侦探先用“望远镜”快速扫视整张图片，找出几个可疑的地方，比如人脸、手或者图片上的文字，这些地方AI造假的可能性比较大。第二步，侦探拿出“放大镜”，仔细观察这些可疑区域的细节，看看有没有不自然的纹理、模糊的文字或者奇怪的光影。通过这种两步走的方式，模型不仅给出这张图是真实还是AI生成的判断，还会告诉你为什么这么认为，并指出具体的可疑部分。为了让侦探更聪明，研究团队还准备了一个专门的数据集，里面的图片都被标注了哪些地方可能是假的，并配有详细的解释。训练过程中，模型学会了如何结合图片和语言描述，像人类专家一样有理有据地做出判定。这样，ZoomIn不仅能准确识别假图，还能让人明白它是怎么判断的，避免了“黑盒”式的神秘感。 
## Creative synthesis of kinematic mechanisms 
2025-09-30｜Columbia U

<u>http://arxiv.org/abs/2510.03308v1</u>  
<u>https://jl6017.github.io/GenMech/</u> 
### 概述 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/114.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/115.jpg)  
本文将平面连杆机构的运动学综合问题重新定义为跨领域的图像生成任务，提出了一种基于图像的统一表达框架。研究团队构建了覆盖多种机构类型（从简单的曲柄摇杆到复杂的八杆机构如詹森机构）的RGB图像数据集，利用图像同时编码机构结构与运动轨迹，并引入颜色渐变表示轨迹点的绘制速度，实现了运动轨迹形状与速度的联合条件合成。该方法通过共享潜在空间的变分自编码器（VAE）模型，能够在机构图像和轨迹图像之间实现双向转换，支持从轨迹生成机构设计及从机构推断运动轨迹。此研究突破了传统依赖图结构或关节坐标列表的限制，实现了跨多种机构类型的泛化。实验验证了该图像生成框架在不同复杂度数据集上的有效性，展示了其在机械设计自动化和机器人运动规划中的潜力。

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/116.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/117.jpg)  
本研究核心方法基于共享潜在空间的变分自编码器（VAE）架构，具体包括以下几个关键部分：  

1. **图像编码与解码**：设计两个编码器分别处理轨迹图像和机构图像，两个解码器对应重建各自输入。轨迹图像通过颜色编码速度信息，机构图像通过预定义颜色区分不同连杆和关节。  
2. **共享潜在空间**：轨迹和机构的编码结果被映射到同一潜在分布，采用正态分布约束，通过KL散度正则化潜在变量，使两域嵌入紧密对齐，实现跨域信息共享。  
3. **损失函数设计**：包括轨迹和机构的重建误差，潜在变量的KL散度，潜在空间对齐损失，以及跨域预测误差，确保模型既能准确复现输入，又能实现轨迹与机构间的有效转换。  
4. **数据集构建**：采用基于三角形操作符递归生成1自由度平面机构，筛选满足设计要求的机构结构，构建涵盖从简单四杆到复杂多环机构的多层次数据集。  
5. **训练与推断**：模型通过端到端训练学习潜在空间表示，推断时可实现轨迹到机构的合成及机构到轨迹的分析，支持双向生成。

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/118.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/119.jpg)  
实验部分采用三个不同复杂度的数据集验证模型性能：包含10万条四杆机构轨迹的简单集，20万条混合四杆与曲柄滑块机构的中等复杂集，以及包含多环复杂机构的子集。实验任务包括轨迹到机构的合成、机构到轨迹的分析，以及两阶段的合成-分析闭环生成。结果显示，基于视觉Transformer（ViT-base）模型在图像重建精度（PSNR指标）上优于传统ResNet和轻量ViT模型，且整体性能随机构复杂度提升而保持稳定。消融实验表明颜色编码、完整损失函数及跨域损失对性能贡献显著。定性展示了模型生成的机构与轨迹高度吻合真实样本，且通过3D打印实物验证了设计的可行性。实验充分证明了图像表示及共享潜在空间VAE框架在机械运动学综合中的有效性和通用性。

### 通俗易懂  
这项研究的核心是用“看图画”的方式来设计机械结构和它们的运动轨迹。想象我们把一个机械装置和它的运动路径都画成彩色图片，图片中的颜色不仅代表形状，还代表运动速度。然后，我们用一种叫“变分自编码器”的智能算法，把机械图和运动图“压缩”成一串数字代码，这些代码在一个共同的空间里，代表了机械和运动的本质信息。算法学会了如何根据运动轨迹的图片生成机械结构的图片，也能反过来根据机械结构的图片预测运动轨迹。这样，我们就能通过画一个想要的运动轨迹，自动“画”出实现它的机械装置，或者给一个机械装置，预测它的运动表现。这个方法就像给机械设计装上了“智能大脑”，既能理解机械的运动，也能创造新的机械结构，极大简化了复杂机械设计的过程。 
# Topic: Image Generation｜Safety 
## Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models 
2025-09-30｜USTC, BUAA, Manchester, NTU, FYUST 

<u>http://arxiv.org/abs/2510.03302v1</u>  
### 概述 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/124.jpg)  
本文聚焦于文本到图像（T2I）扩散模型中的概念擦除技术，这类技术通过抑制模型生成不当或受版权保护内容来保障安全。然而，随着模型架构从传统的UNet向基于Flow Matching和Transformer的“Flux”家族演进，现有的擦除方法（如ESD、UCE、AC）效果显著下降。研究发现，所谓的“擦除”并非真正删除知识，而是通过修改模型权重引导采样轨迹偏离特定概念区域，形成一种“假性失忆”。基于此，作者提出了RevAm框架，利用强化学习（RL）动态调整采样过程中噪声预测的速度场，逆转擦除效果，实现被擦除概念的恢复。该方法无需修改模型权重，通过对采样轨迹的精细控制，显著提升恢复质量并大幅缩短计算时间，暴露了当前安全机制的根本脆弱性，呼吁开发更为稳健的知识移除技术。

### 方法 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/125.jpg)  
RevAm的核心在于将概念恢复视为一个序贯决策问题，通过RL优化采样轨迹的速度场调整策略。具体包括以下关键步骤：  

1. **速度场干预建模**：在每个采样步骤，模型预测的速度向量（噪声方向和幅度）被政策网络调整。动作由两个参数构成：幅度缩放ρ和方向旋转ϕ，动作空间被限制在小范围内以保证稳定性。  
2. **语义子空间旋转**：旋转操作在由当前速度向量和一个基于无条件与条件速度差的参考方向构成的二维子空间内执行，确保调整聚焦于与目标概念相关的语义方向。  
3. **强化学习训练**：采用Group Relative Policy Optimization（GRPO）算法，利用多样化的图像质量和概念恢复奖励信号，进行批量采样并基于群体相对优势更新策略网络，提升训练稳定性和效率。  
4. **无权重修改的推理时控制**：策略在推理阶段动态调整速度场，无需修改模型本体，实现在生成过程中实时引导采样轨迹回归被擦除的概念区域。  

### 实验 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/126.jpg) 
![](http://www.huyaoqi.ip-ddns.com:83/C%3A/Users/13756/Documents/GitHub/paper_daily_format/paper_daily_back_flask/result/2025-10-10/127.jpg)  
作者在Flux.1 [dev]模型及多种主流概念擦除方法（如ESD、AC、EA、EAP）上进行了全面评估，涵盖NSFW内容（裸露、暴力）、艺术风格（梵高、毕加索）、实体、抽象概念及关系等多个类别。实验结果显示，RevAm在攻击成功率（ASR）上显著超越现有方法，恢复被擦除的概念更加精准且多样，且计算效率提升约10倍。此外，消融实验验证了联合优化幅度和方向参数及双重奖励设计的必要性，进一步提升了恢复速度和质量。定量指标和视觉示例均表明RevAm在各种复杂概念恢复任务中表现出更强的泛化能力和鲁棒性，揭示了现有擦除策略的根本缺陷，强调了未来开发真正“删除”知识的必要性。

### 通俗易懂  
简单来说，传统的概念擦除方法并没有真正把不想要的内容从模型里“删掉”，而是让模型在生成图片时“绕开”这些内容。就好比在地图上绕路，而不是把路堵死。RevAm的做法是用一个智能“导航员”来调整模型生成图片时的路线，让它重新走回被“绕开”的地方，从而“找回”被擦除的内容。这个导航员通过观察每一步生成的图片，决定下一步该往哪个方向走、走多远，动作很小但非常精准。它通过强化学习不断学习和优化自己的“导航策略”，学会怎么最快最准确地恢复被擦除的内容。这样，RevAm不用改模型本身，只在生成过程中实时调整，既高效又灵活。比起以前慢慢试错找“绕路”的方法，RevAm更聪明也更快，大大提升了恢复效果。 